{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Bare Earth + Official SpectralGPT Precompute (Colab GPU)\n",
    "\n",
    "Use this notebook first.\n",
    "It uses `data/processed/features_normalized_points.csv` as the source training table,\n",
    "samples GA Barest Earth for each point, runs official SpectralGPT on the Bare Earth pixels,\n",
    "and builds a fused `feat_*` table for ResNet training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime\n",
    "\n",
    "In Colab, set `Runtime -> Change runtime type -> T4/A100 GPU`, then run all cells top to bottom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"[1/9] Mount + clone repo\")\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "USE_GIT_CLONE = True\n",
    "REPO_GIT_URL = \"https://github.com/JackOnThePaddock/soil-resnet-model.git\"\n",
    "DRIVE_REPO_DIR = \"/content/drive/MyDrive/soil-resnet-model\"\n",
    "PROJECT_DIR = \"/content/soil-resnet-model\"\n",
    "\n",
    "if os.path.exists(PROJECT_DIR):\n",
    "    shutil.rmtree(PROJECT_DIR)\n",
    "\n",
    "if USE_GIT_CLONE:\n",
    "    subprocess.run([\"git\", \"clone\", REPO_GIT_URL, PROJECT_DIR], check=True)\n",
    "else:\n",
    "    if not os.path.exists(DRIVE_REPO_DIR):\n",
    "        raise FileNotFoundError(f\"Repo not found at {DRIVE_REPO_DIR}\")\n",
    "    shutil.copytree(DRIVE_REPO_DIR, PROJECT_DIR)\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(\"Project dir:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[2/9] Install dependencies\")\n",
    "!python -V\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[3/9] Check GPU\")\n",
    "import torch\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[4/9] Configure paths\")\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "WCS_WORKERS = 16\n",
    "WCS_TIMEOUT = 120\n",
    "WCS_RETRIES = 3\n",
    "SPECTRAL_DIM = 16\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "paths = {\n",
    "    \"raw\": Path(\"data/processed/features.csv\"),\n",
    "    \"normalized\": Path(\"data/processed/features_normalized.csv\"),\n",
    "    \"normalized_points\": Path(\"data/processed/features_normalized_points.csv\"),\n",
    "    \"be_sgpt\": Path(\"data/processed/features_normalized_points_be_sgpt.csv\"),\n",
    "    \"sgpt_only\": Path(\"data/processed/features_normalized_points_sgpt_embeddings.csv\"),\n",
    "    \"sgpt_raw\": Path(\"data/processed/features_normalized_points_sgpt_official_raw.csv\"),\n",
    "    \"fused_feat\": Path(\"data/processed/features_normalized_points_fused_feat.csv\"),\n",
    "    \"fused_meta\": Path(\"data/processed/features_normalized_points_fused_meta.json\"),\n",
    "}\n",
    "\n",
    "if not paths[\"normalized_points\"].exists():\n",
    "    print(\"normalized_points missing; attempting to build from features.csv + features_normalized.csv\")\n",
    "    for req in [paths[\"raw\"], paths[\"normalized\"]]:\n",
    "        if not req.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Missing required input: {req}. Cannot build {paths['normalized_points']}.\"\n",
    "            )\n",
    "\n",
    "    raw_meta = pd.read_csv(paths[\"raw\"], usecols=[\"id\", \"lat\", \"lon\"])\n",
    "    norm_df = pd.read_csv(paths[\"normalized\"])\n",
    "    if len(raw_meta) != len(norm_df):\n",
    "        raise ValueError(f\"Row mismatch raw={len(raw_meta)} normalized={len(norm_df)}\")\n",
    "\n",
    "    norm_points = pd.concat([raw_meta.reset_index(drop=True), norm_df.reset_index(drop=True)], axis=1)\n",
    "    paths[\"normalized_points\"].parent.mkdir(parents=True, exist_ok=True)\n",
    "    norm_points.to_csv(paths[\"normalized_points\"], index=False)\n",
    "    print(\"Created\", paths[\"normalized_points\"], norm_points.shape)\n",
    "\n",
    "print(\"Input ready:\", paths[\"normalized_points\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[5/9] Validate normalized points schema\")\n",
    "points_df = pd.read_csv(paths[\"normalized_points\"])\n",
    "required_cols = [\"id\", \"lat\", \"lon\"] + [f\"band_{i}\" for i in range(64)]\n",
    "missing = [c for c in required_cols if c not in points_df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"normalized_points is missing columns: {missing[:10]}\")\n",
    "\n",
    "print(\"Rows:\", len(points_df))\n",
    "print(\"Columns:\", len(points_df.columns))\n",
    "print(\"Band columns:\", len([c for c in points_df.columns if c.startswith('band_')]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[6/9] Pull Bare Earth + official SpectralGPT embeddings\")\n",
    "cmd = [\n",
    "    \"python\", \"scripts/pull_bare_earth_embeddings.py\",\n",
    "    \"--normalized-csv\", str(paths[\"normalized_points\"]),\n",
    "    \"--output-csv\", str(paths[\"be_sgpt\"]),\n",
    "    \"--output-embeddings-csv\", str(paths[\"sgpt_only\"]),\n",
    "    \"--output-official-raw-csv\", str(paths[\"sgpt_raw\"]),\n",
    "    \"--workers\", str(WCS_WORKERS),\n",
    "    \"--timeout\", str(WCS_TIMEOUT),\n",
    "    \"--retries\", str(WCS_RETRIES),\n",
    "    \"--spectral-backend\", \"official_pretrained\",\n",
    "    \"--official-request-chunk-size\", \"64\",\n",
    "    \"--spectral-dim\", str(SPECTRAL_DIM),\n",
    "    \"--seed\", str(RANDOM_SEED),\n",
    "]\n",
    "print(\"Running:\", \" \".join(cmd))\n",
    "subprocess.run(cmd, check=True)\n",
    "print(\"Saved\", paths[\"be_sgpt\"])\n",
    "print(\"Saved\", paths[\"sgpt_only\"])\n",
    "print(\"Saved\", paths[\"sgpt_raw\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[7/9] Build fused feat_* training table\")\n",
    "fused = pd.read_csv(paths[\"be_sgpt\"])\n",
    "\n",
    "def sort_numeric_suffix(cols):\n",
    "    def key(c):\n",
    "        m = re.search(r\"(\\d+)$\", c)\n",
    "        return (0, int(m.group(1)), c) if m else (1, -1, c)\n",
    "    return sorted(cols, key=key)\n",
    "\n",
    "band_cols = sort_numeric_suffix([c for c in fused.columns if c.lower().startswith(\"band_\")])\n",
    "be_cols = sort_numeric_suffix([c for c in fused.columns if c.lower().startswith(\"be_\")])\n",
    "sgpt_cols = sort_numeric_suffix([c for c in fused.columns if c.lower().startswith(\"sgpt_\")])\n",
    "\n",
    "if len(band_cols) != 64:\n",
    "    raise ValueError(f\"Expected 64 band cols, got {len(band_cols)}\")\n",
    "if len(sgpt_cols) == 0:\n",
    "    raise ValueError(\"No sgpt_* columns found\")\n",
    "\n",
    "source_cols = band_cols + be_cols + sgpt_cols\n",
    "for i, c in enumerate(source_cols):\n",
    "    fused[f\"feat_{i:03d}\"] = fused[c]\n",
    "\n",
    "paths[\"fused_feat\"].parent.mkdir(parents=True, exist_ok=True)\n",
    "fused.to_csv(paths[\"fused_feat\"], index=False)\n",
    "\n",
    "meta = {\n",
    "    \"alpha_cols\": band_cols,\n",
    "    \"bareearth_cols\": be_cols,\n",
    "    \"spectral_cols\": sgpt_cols,\n",
    "    \"feat_cols\": [f\"feat_{i:03d}\" for i in range(len(source_cols))],\n",
    "}\n",
    "paths[\"fused_meta\"].write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved\", paths[\"fused_feat\"], fused.shape)\n",
    "print(\"Counts:\", {\"band\": len(band_cols), \"be\": len(be_cols), \"sgpt\": len(sgpt_cols), \"feat\": len(meta[\"feat_cols\"])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[8/9] Sanity checks\")\n",
    "import numpy as np\n",
    "\n",
    "fused = pd.read_csv(paths[\"fused_feat\"])\n",
    "feat_cols = [c for c in fused.columns if c.startswith(\"feat_\")]\n",
    "\n",
    "arr = fused[feat_cols].to_numpy(dtype=np.float64)\n",
    "print(\"fused rows:\", len(fused), \"features:\", len(feat_cols))\n",
    "print(\"non-finite values:\", int((~np.isfinite(arr)).sum()))\n",
    "\n",
    "print(\"sample columns:\", feat_cols[:5], \"...\", feat_cols[-5:])\n",
    "fused[[\"id\", \"lat\", \"lon\"] + feat_cols[:3]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[9/9] Save outputs to Drive\")\n",
    "from datetime import datetime\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "drive_out = Path(f\"/content/drive/MyDrive/soil-resnet-outputs/step1_bareearth_sgpt_{stamp}\")\n",
    "drive_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "copy_targets = [\n",
    "    paths[\"normalized_points\"],\n",
    "    paths[\"be_sgpt\"],\n",
    "    paths[\"sgpt_only\"],\n",
    "    paths[\"sgpt_raw\"],\n",
    "    paths[\"fused_feat\"],\n",
    "    paths[\"fused_meta\"],\n",
    "]\n",
    "\n",
    "for p in copy_targets:\n",
    "    if p.exists():\n",
    "        shutil.copy2(p, drive_out / p.name)\n",
    "\n",
    "print(\"Saved to:\", drive_out)\n",
    "print(\"Use this folder in Step 2 notebook as DRIVE_PRECOMPUTED_DIR\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
