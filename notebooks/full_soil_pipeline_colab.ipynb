{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full Colab Pipeline: Normalized Data + Bare Earth + SpectralGPT + ResNet + Validation\n",
        "\n",
        "This notebook runs your complete workflow on Colab GPU:\n",
        "1. Prepare full normalized training data with coordinates.\n",
        "2. Pull GA Barest Earth (Sentinel-2) for all training points.\n",
        "3. Train SpectralGPT embeddings.\n",
        "4. Train ResNet ensembles (`alpha-only` and `fused`).\n",
        "5. Produce cross-validation and independent validation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data rights note\n",
        "\n",
        "This notebook uses DEA/Geoscience Australia OWS endpoints for Bare Earth. Ensure your use complies with DEA/GA usage rights and constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Runtime setup\n",
        "\n",
        "In Colab, set `Runtime -> Change runtime type -> GPU` before running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ---- Configure these paths ----\n",
        "USE_GIT_CLONE = True\n",
        "REPO_GIT_URL = \"https://github.com/JackOnThePaddock/soil-resnet-model.git\"  # e.g. \"https://github.com/<user>/soil-resnet-model.git\"\n",
        "DRIVE_REPO_DIR = \"/content/drive/MyDrive/soil-resnet-model\"\n",
        "PROJECT_DIR = \"/content/soil-resnet-model\"\n",
        "\n",
        "if os.path.exists(PROJECT_DIR):\n",
        "    shutil.rmtree(PROJECT_DIR)\n",
        "\n",
        "if USE_GIT_CLONE:\n",
        "    if not REPO_GIT_URL:\n",
        "        raise ValueError(\"Set REPO_GIT_URL or set USE_GIT_CLONE=False\")\n",
        "    subprocess.run([\"git\", \"clone\", REPO_GIT_URL, PROJECT_DIR], check=True)\n",
        "else:\n",
        "    if not os.path.exists(DRIVE_REPO_DIR):\n",
        "        raise FileNotFoundError(f\"Repo not found at {DRIVE_REPO_DIR}\")\n",
        "    shutil.copytree(DRIVE_REPO_DIR, PROJECT_DIR)\n",
        "\n",
        "os.chdir(PROJECT_DIR)\n",
        "print(\"Project dir:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -V\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Run configuration\n",
        "\n",
        "Set `FAST_MODE=False` for full training. `FAST_MODE=True` is useful only for quick pipeline checks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "FAST_MODE = False\n",
        "RUN_ALPHA = True\n",
        "RUN_FUSED = True\n",
        "RUN_BARE_EARTH_PULL = True\n",
        "RUN_INDEPENDENT_VALIDATION = True\n",
        "\n",
        "SPECTRAL_DIM = 16\n",
        "WCS_WORKERS = 16\n",
        "WCS_TIMEOUT = 120\n",
        "WCS_RETRIES = 3\n",
        "\n",
        "TRAINING = {\n",
        "    \"ensemble_size\": 3 if FAST_MODE else 5,\n",
        "    \"n_splits\": 3 if FAST_MODE else 5,\n",
        "    \"epochs_alpha\": 60 if FAST_MODE else 300,\n",
        "    \"final_epochs_alpha\": 40 if FAST_MODE else 200,\n",
        "    \"epochs_fused\": 80 if FAST_MODE else 350,\n",
        "    \"final_epochs_fused\": 60 if FAST_MODE else 220,\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"patience\": 20 if FAST_MODE else 35,\n",
        "    \"lr_patience\": 10 if FAST_MODE else 20,\n",
        "    \"lr_factor\": 0.5,\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"random_seed\": 42,\n",
        "}\n",
        "\n",
        "TARGETS = [\"ph\", \"cec\", \"esp\", \"soc\", \"ca\", \"mg\", \"na\"]\n",
        "\n",
        "paths = {\n",
        "    \"raw\": Path(\"data/processed/features.csv\"),\n",
        "    \"normalized\": Path(\"data/processed/features_normalized.csv\"),\n",
        "    \"normalized_points\": Path(\"data/processed/features_normalized_points.csv\"),\n",
        "    \"be_sgpt\": Path(\"data/processed/features_normalized_bareearth_sgpt.csv\"),\n",
        "    \"sgpt_only\": Path(\"data/processed/features_normalized_sgpt_embeddings.csv\"),\n",
        "    \"fused_feat\": Path(\"data/processed/features_normalized_fused_feat.csv\"),\n",
        "    \"fused_meta\": Path(\"data/processed/features_normalized_fused_meta.json\"),\n",
        "    \"cfg_alpha\": Path(\"configs/colab_resnet_alpha_normalized.yaml\"),\n",
        "    \"cfg_fused\": Path(\"configs/colab_resnet_fused_normalized.yaml\"),\n",
        "    \"model_alpha\": Path(\"models/colab_resnet_alpha_norm\"),\n",
        "    \"model_fused\": Path(\"models/colab_resnet_fused_norm\"),\n",
        "    \"validation\": Path(\"data/validation/national_independent_1368.csv\"),\n",
        "}\n",
        "\n",
        "for k, p in paths.items():\n",
        "    if k in {\"raw\", \"normalized\"} and not p.exists():\n",
        "        raise FileNotFoundError(f\"Missing required file: {p}\")\n",
        "\n",
        "print(\"Config loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Build normalized training table with point coordinates\n",
        "\n",
        "`features_normalized.csv` has normalized targets + AlphaEarth features, but no `lat/lon`. This cell joins `id/lat/lon` from `features.csv` by row order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_meta = pd.read_csv(paths[\"raw\"], usecols=[\"id\", \"lat\", \"lon\"])\n",
        "norm_df = pd.read_csv(paths[\"normalized\"])\n",
        "\n",
        "if len(raw_meta) != len(norm_df):\n",
        "    raise ValueError(f\"Row mismatch: raw={len(raw_meta)} normalized={len(norm_df)}\")\n",
        "\n",
        "norm_points = pd.concat([raw_meta.reset_index(drop=True), norm_df.reset_index(drop=True)], axis=1)\n",
        "paths[\"normalized_points\"].parent.mkdir(parents=True, exist_ok=True)\n",
        "norm_points.to_csv(paths[\"normalized_points\"], index=False)\n",
        "\n",
        "print(\"Saved:\", paths[\"normalized_points\"], norm_points.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Pull Bare Earth and train SpectralGPT embeddings on all normalized rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "if RUN_BARE_EARTH_PULL:\n",
        "    cmd = [\n",
        "        \"python\", \"scripts/pull_bare_earth_embeddings.py\",\n",
        "        \"--normalized-csv\", str(paths[\"normalized\"]),\n",
        "        \"--points-csv\", str(paths[\"raw\"]),\n",
        "        \"--output-csv\", str(paths[\"be_sgpt\"]),\n",
        "        \"--output-embeddings-csv\", str(paths[\"sgpt_only\"]),\n",
        "        \"--workers\", str(WCS_WORKERS),\n",
        "        \"--timeout\", str(WCS_TIMEOUT),\n",
        "        \"--retries\", str(WCS_RETRIES),\n",
        "        \"--spectral-backend\", \"official_pretrained\",\n",
        "        \"--official-request-chunk-size\", \"64\",\n",
        "        \"--output-official-raw-csv\", \"data/processed/features_normalized_sgpt_official_raw.csv\",\n",
        "        \"--spectral-dim\", str(SPECTRAL_DIM),\n",
        "        \"--seed\", str(TRAINING[\"random_seed\"]),\n",
        "    ]\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "else:\n",
        "    if not paths[\"be_sgpt\"].exists():\n",
        "        raise FileNotFoundError(f\"Expected existing file: {paths['be_sgpt']}\")\n",
        "\n",
        "print(\"Bare Earth + SGPT file:\", paths[\"be_sgpt\"].exists())\n",
        "print(\"Embeddings-only file:\", paths[\"sgpt_only\"].exists())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Build fused `feat_*` columns for ResNet fused training\n",
        "\n",
        "Training script expects one feature namespace (e.g. `feat_000..`). We pack:\n",
        "- AlphaEarth: `band_*`\n",
        "- Bare Earth: `be_*`\n",
        "- SpectralGPT: `sgpt_*`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "fused = pd.read_csv(paths[\"be_sgpt\"])\n",
        "\n",
        "\n",
        "def sort_numeric_suffix(cols):\n",
        "    def key(c):\n",
        "        m = re.search(r\"(\\d+)$\", c)\n",
        "        return (0, int(m.group(1)), c) if m else (1, -1, c)\n",
        "    return sorted(cols, key=key)\n",
        "\n",
        "band_cols = sort_numeric_suffix([c for c in fused.columns if c.lower().startswith(\"band_\")])\n",
        "be_cols = sort_numeric_suffix([c for c in fused.columns if c.lower().startswith(\"be_\")])\n",
        "sgpt_cols = sort_numeric_suffix([c for c in fused.columns if c.lower().startswith(\"sgpt_\")])\n",
        "\n",
        "if len(band_cols) != 64:\n",
        "    raise ValueError(f\"Expected 64 alpha band columns, got {len(band_cols)}\")\n",
        "if len(be_cols) < 5:\n",
        "    raise ValueError(f\"Too few Bare Earth columns detected: {len(be_cols)}\")\n",
        "if len(sgpt_cols) == 0:\n",
        "    raise ValueError(\"No SpectralGPT columns found\")\n",
        "\n",
        "source_cols = band_cols + be_cols + sgpt_cols\n",
        "for i, col in enumerate(source_cols):\n",
        "    fused[f\"feat_{i:03d}\"] = fused[col]\n",
        "\n",
        "paths[\"fused_feat\"].parent.mkdir(parents=True, exist_ok=True)\n",
        "fused.to_csv(paths[\"fused_feat\"], index=False)\n",
        "\n",
        "meta = {\n",
        "    \"alpha_cols\": band_cols,\n",
        "    \"bareearth_cols\": be_cols,\n",
        "    \"spectral_cols\": sgpt_cols,\n",
        "    \"feat_cols\": [f\"feat_{i:03d}\" for i in range(len(source_cols))],\n",
        "}\n",
        "paths[\"fused_meta\"].write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"Saved fused table:\", paths[\"fused_feat\"], fused.shape)\n",
        "print(\"Feature blocks:\", {\"band\": len(band_cols), \"be\": len(be_cols), \"sgpt\": len(sgpt_cols), \"feat_total\": len(meta['feat_cols'])})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Write Colab training configs (normalized targets)\n",
        "\n",
        "Important: because targets are already normalized, all target transforms are set to `identity`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "paths[\"cfg_alpha\"].parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "config_alpha = {\n",
        "    \"model\": {\n",
        "        \"input_dim\": 64,\n",
        "        \"hidden_dim\": 128,\n",
        "        \"num_blocks\": 2,\n",
        "        \"dropout\": 0.2,\n",
        "        \"activation\": \"silu\",\n",
        "    },\n",
        "    \"targets\": [\"pH\", \"CEC\", \"ESP\", \"SOC\", \"Ca\", \"Mg\", \"Na\"],\n",
        "    \"training\": {\n",
        "        \"ensemble_size\": TRAINING[\"ensemble_size\"],\n",
        "        \"n_splits\": TRAINING[\"n_splits\"],\n",
        "        \"epochs\": TRAINING[\"epochs_alpha\"],\n",
        "        \"final_epochs\": TRAINING[\"final_epochs_alpha\"],\n",
        "        \"batch_size\": TRAINING[\"batch_size\"],\n",
        "        \"learning_rate\": TRAINING[\"learning_rate\"],\n",
        "        \"weight_decay\": TRAINING[\"weight_decay\"],\n",
        "        \"patience\": TRAINING[\"patience\"],\n",
        "        \"lr_patience\": TRAINING[\"lr_patience\"],\n",
        "        \"lr_factor\": TRAINING[\"lr_factor\"],\n",
        "        \"grad_clip\": TRAINING[\"grad_clip\"],\n",
        "        \"cv_strategy\": \"group_kfold\",\n",
        "        \"loss_name\": \"weighted_huber\",\n",
        "        \"huber_delta\": 1.0,\n",
        "        \"esp_consistency_weight\": 0.05,\n",
        "        \"target_weight_mode\": \"inverse_frequency\",\n",
        "        \"sample_weight_mode\": \"rare_target_average\",\n",
        "        \"auto_target_transforms\": False,\n",
        "        \"target_transforms\": {t: \"identity\" for t in TARGETS},\n",
        "        \"specialist_targets\": [\"cec\", \"esp\", \"soc\"],\n",
        "        \"specialist_epochs\": 80 if FAST_MODE else 120,\n",
        "        \"specialist_patience\": 15 if FAST_MODE else 20,\n",
        "        \"specialist_val_fraction\": 0.2,\n",
        "        \"specialist_blend_weight\": 0.4,\n",
        "        \"random_seed\": TRAINING[\"random_seed\"],\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"feature_prefix\": \"band_\",\n",
        "        \"n_features\": 64,\n",
        "        \"group_by\": \"latlon\",\n",
        "        \"group_round\": 4,\n",
        "        \"reference_data\": None,\n",
        "    },\n",
        "    \"output\": {\n",
        "        \"model_dir\": str(paths[\"model_alpha\"]),\n",
        "        \"metrics_dir\": \"results/metrics\",\n",
        "        \"scaler_file\": \"scaler.pkl\",\n",
        "    },\n",
        "}\n",
        "\n",
        "fused_cols_df = pd.read_csv(paths[\"fused_feat\"], nrows=1)\n",
        "n_fused_features = len([c for c in fused_cols_df.columns if c.startswith(\"feat_\")])\n",
        "\n",
        "config_fused = {\n",
        "    \"model\": {\n",
        "        \"input_dim\": n_fused_features,\n",
        "        \"hidden_dim\": 192,\n",
        "        \"num_blocks\": 3,\n",
        "        \"dropout\": 0.2,\n",
        "        \"activation\": \"silu\",\n",
        "    },\n",
        "    \"targets\": [\"pH\", \"CEC\", \"ESP\", \"SOC\", \"Ca\", \"Mg\", \"Na\"],\n",
        "    \"training\": {\n",
        "        \"ensemble_size\": TRAINING[\"ensemble_size\"],\n",
        "        \"n_splits\": TRAINING[\"n_splits\"],\n",
        "        \"epochs\": TRAINING[\"epochs_fused\"],\n",
        "        \"final_epochs\": TRAINING[\"final_epochs_fused\"],\n",
        "        \"batch_size\": TRAINING[\"batch_size\"],\n",
        "        \"learning_rate\": TRAINING[\"learning_rate\"],\n",
        "        \"weight_decay\": TRAINING[\"weight_decay\"],\n",
        "        \"patience\": TRAINING[\"patience\"],\n",
        "        \"lr_patience\": TRAINING[\"lr_patience\"],\n",
        "        \"lr_factor\": TRAINING[\"lr_factor\"],\n",
        "        \"grad_clip\": TRAINING[\"grad_clip\"],\n",
        "        \"cv_strategy\": \"group_kfold\",\n",
        "        \"loss_name\": \"weighted_huber\",\n",
        "        \"huber_delta\": 1.0,\n",
        "        \"esp_consistency_weight\": 0.08,\n",
        "        \"target_weight_mode\": \"inverse_frequency\",\n",
        "        \"sample_weight_mode\": \"rare_target_average\",\n",
        "        \"auto_target_transforms\": False,\n",
        "        \"target_transforms\": {t: \"identity\" for t in TARGETS},\n",
        "        \"specialist_targets\": [\"cec\", \"esp\", \"soc\"],\n",
        "        \"specialist_epochs\": 100 if FAST_MODE else 150,\n",
        "        \"specialist_patience\": 15 if FAST_MODE else 25,\n",
        "        \"specialist_val_fraction\": 0.2,\n",
        "        \"specialist_blend_weight\": 0.4,\n",
        "        \"random_seed\": TRAINING[\"random_seed\"],\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"feature_prefix\": \"feat_\",\n",
        "        \"n_features\": None,\n",
        "        \"group_by\": \"latlon\",\n",
        "        \"group_round\": 4,\n",
        "        \"reference_data\": None,\n",
        "    },\n",
        "    \"output\": {\n",
        "        \"model_dir\": str(paths[\"model_fused\"]),\n",
        "        \"metrics_dir\": \"results/metrics\",\n",
        "        \"scaler_file\": \"scaler.pkl\",\n",
        "    },\n",
        "}\n",
        "\n",
        "paths[\"cfg_alpha\"].write_text(yaml.safe_dump(config_alpha, sort_keys=False), encoding=\"utf-8\")\n",
        "paths[\"cfg_fused\"].write_text(yaml.safe_dump(config_fused, sort_keys=False), encoding=\"utf-8\")\n",
        "\n",
        "print(\"Wrote\", paths[\"cfg_alpha\"])\n",
        "print(\"Wrote\", paths[\"cfg_fused\"])\n",
        "print(\"Fused input_dim:\", n_fused_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Train ResNet ensemble: alpha-only (normalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "if RUN_ALPHA:\n",
        "    cmd = [\n",
        "        \"python\", \"scripts/train_resnet_ensemble.py\",\n",
        "        \"--data\", str(paths[\"normalized_points\"]),\n",
        "        \"--config\", str(paths[\"cfg_alpha\"]),\n",
        "        \"--output\", str(paths[\"model_alpha\"]),\n",
        "        \"--cv-strategy\", \"group_kfold\",\n",
        "        \"--seed\", str(TRAINING[\"random_seed\"]),\n",
        "    ]\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "else:\n",
        "    print(\"Skipped alpha training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Train ResNet ensemble: fused (normalized + Bare Earth + SpectralGPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_FUSED:\n",
        "    cmd = [\n",
        "        \"python\", \"scripts/train_resnet_ensemble.py\",\n",
        "        \"--data\", str(paths[\"fused_feat\"]),\n",
        "        \"--config\", str(paths[\"cfg_fused\"]),\n",
        "        \"--output\", str(paths[\"model_fused\"]),\n",
        "        \"--cv-strategy\", \"group_kfold\",\n",
        "        \"--seed\", str(TRAINING[\"random_seed\"]),\n",
        "    ]\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "else:\n",
        "    print(\"Skipped fused training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Cross-validation error summary from trained ensembles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def summarize_ensemble_metrics(csv_path: Path, model_label: str) -> pd.DataFrame:\n",
        "    if not csv_path.exists():\n",
        "        return pd.DataFrame()\n",
        "    df = pd.read_csv(csv_path)\n",
        "    rows = []\n",
        "    for col in df.columns:\n",
        "        m = re.match(r\"^(.*)_(r2|rmse)$\", col)\n",
        "        if not m:\n",
        "            continue\n",
        "        target, metric = m.group(1), m.group(2)\n",
        "        rows.append({\n",
        "            \"model\": model_label,\n",
        "            \"target\": target,\n",
        "            \"metric\": metric,\n",
        "            \"mean\": float(df[col].mean()),\n",
        "            \"std\": float(df[col].std(ddof=0)),\n",
        "        })\n",
        "    out = pd.DataFrame(rows)\n",
        "    if out.empty:\n",
        "        return out\n",
        "    out[\"target\"] = out[\"target\"].str.lower()\n",
        "    return out.sort_values([\"model\", \"target\", \"metric\"]).reset_index(drop=True)\n",
        "\n",
        "alpha_metrics = summarize_ensemble_metrics(paths[\"model_alpha\"] / \"ensemble_metrics.csv\", \"alpha_norm\")\n",
        "fused_metrics = summarize_ensemble_metrics(paths[\"model_fused\"] / \"ensemble_metrics.csv\", \"fused_norm\")\n",
        "cv_summary = pd.concat([alpha_metrics, fused_metrics], ignore_index=True)\n",
        "\n",
        "if cv_summary.empty:\n",
        "    print(\"No ensemble metrics found\")\n",
        "else:\n",
        "    display(cv_summary)\n",
        "\n",
        "pivot = cv_summary.pivot_table(index=[\"target\", \"metric\"], columns=\"model\", values=\"mean\", aggfunc=\"first\") if not cv_summary.empty else pd.DataFrame()\n",
        "if not pivot.empty:\n",
        "    print(\"\\nMean CV metrics\")\n",
        "    display(pivot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Independent validation (alpha model) on `national_independent_1368.csv`\n",
        "\n",
        "This validation file has AlphaEarth `A00..A63` and target subset (`ph`, `cec_cmolkg`, `esp_pct`, `na_cmolkg`).\n",
        "Because this notebook trains on normalized targets, predictions are converted back to raw units before metric reporting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_INDEPENDENT_VALIDATION:\n",
        "    from src.models.ensemble import SoilEnsemble\n",
        "    from src.evaluation.metrics import compute_metrics\n",
        "\n",
        "    if not paths[\"validation\"].exists():\n",
        "        raise FileNotFoundError(f\"Missing validation file: {paths['validation']}\")\n",
        "\n",
        "    val_df = pd.read_csv(paths[\"validation\"])\n",
        "\n",
        "    # Map A00..A63 -> band_0..band_63 expected by alpha model.\n",
        "    for i in range(64):\n",
        "        src = f\"A{i:02d}\"\n",
        "        if src in val_df.columns:\n",
        "            val_df[f\"band_{i}\"] = val_df[src]\n",
        "\n",
        "    feature_cols = [f\"band_{i}\" for i in range(64)]\n",
        "    missing_feat = [c for c in feature_cols if c not in val_df.columns]\n",
        "    if missing_feat:\n",
        "        raise ValueError(f\"Validation features missing: {missing_feat[:5]}\")\n",
        "\n",
        "    model = SoilEnsemble(paths[\"model_alpha\"])\n",
        "    pred_norm, pred_std_norm = model.predict_batch(val_df[feature_cols].values.astype(np.float32))\n",
        "\n",
        "    # Recover normalization stats from raw training targets.\n",
        "    train_raw = pd.read_csv(paths[\"raw\"], usecols=TARGETS)\n",
        "    mu = train_raw.mean(axis=0)\n",
        "    sigma = train_raw.std(axis=0, ddof=0).replace(0.0, 1.0)\n",
        "\n",
        "    pred_norm_df = pd.DataFrame(pred_norm, columns=model.target_names)\n",
        "    pred_raw_df = pred_norm_df.copy()\n",
        "    for t in model.target_names:\n",
        "        pred_raw_df[t] = pred_norm_df[t] * float(sigma[t]) + float(mu[t])\n",
        "\n",
        "    y_true_raw = pd.DataFrame(np.nan, index=val_df.index, columns=model.target_names)\n",
        "    truth_map = {\n",
        "        \"ph\": \"ph\",\n",
        "        \"cec\": \"cec_cmolkg\",\n",
        "        \"esp\": \"esp_pct\",\n",
        "        \"na\": \"na_cmolkg\",\n",
        "    }\n",
        "    for t, src_col in truth_map.items():\n",
        "        if src_col in val_df.columns:\n",
        "            y_true_raw[t] = pd.to_numeric(val_df[src_col], errors=\"coerce\")\n",
        "\n",
        "    metrics = compute_metrics(\n",
        "        y_true=y_true_raw[model.target_names].values,\n",
        "        y_pred=pred_raw_df[model.target_names].values,\n",
        "        target_names=model.target_names,\n",
        "    )\n",
        "\n",
        "    if metrics:\n",
        "        indep_df = pd.DataFrame(metrics).T.reset_index().rename(columns={\"index\": \"target\"})\n",
        "        indep_df = indep_df.sort_values(\"target\").reset_index(drop=True)\n",
        "        display(indep_df)\n",
        "    else:\n",
        "        print(\"No comparable targets found for independent validation\")\n",
        "\n",
        "    # Optional scatter preview for targets that exist in validation file.\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    available_targets = [t for t in [\"ph\", \"cec\", \"esp\", \"na\"] if t in metrics]\n",
        "    if available_targets:\n",
        "        fig, axes = plt.subplots(1, len(available_targets), figsize=(5 * len(available_targets), 4))\n",
        "        if len(available_targets) == 1:\n",
        "            axes = [axes]\n",
        "        for ax, t in zip(axes, available_targets):\n",
        "            mask = np.isfinite(y_true_raw[t].values) & np.isfinite(pred_raw_df[t].values)\n",
        "            ax.scatter(y_true_raw[t].values[mask], pred_raw_df[t].values[mask], s=8, alpha=0.5)\n",
        "            lo = np.nanmin(y_true_raw[t].values[mask])\n",
        "            hi = np.nanmax(y_true_raw[t].values[mask])\n",
        "            ax.plot([lo, hi], [lo, hi], \"k--\", linewidth=1)\n",
        "            ax.set_title(f\"{t} (R2={metrics[t]['r2']:.3f})\")\n",
        "            ax.set_xlabel(\"Observed\")\n",
        "            ax.set_ylabel(\"Predicted\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"Skipped independent validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Save outputs back to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import shutil\n",
        "\n",
        "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "drive_out = Path(f\"/content/drive/MyDrive/soil-resnet-outputs/full_pipeline_{stamp}\")\n",
        "drive_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "copy_targets = [\n",
        "    paths[\"normalized_points\"],\n",
        "    paths[\"be_sgpt\"],\n",
        "    paths[\"sgpt_only\"],\n",
        "    paths[\"fused_feat\"],\n",
        "    paths[\"fused_meta\"],\n",
        "    paths[\"cfg_alpha\"],\n",
        "    paths[\"cfg_fused\"],\n",
        "]\n",
        "for p in copy_targets:\n",
        "    if p.exists():\n",
        "        dest = drive_out / p.name\n",
        "        shutil.copy2(p, dest)\n",
        "\n",
        "if paths[\"model_alpha\"].exists():\n",
        "    shutil.copytree(paths[\"model_alpha\"], drive_out / paths[\"model_alpha\"].name)\n",
        "if paths[\"model_fused\"].exists():\n",
        "    shutil.copytree(paths[\"model_fused\"], drive_out / paths[\"model_fused\"].name)\n",
        "\n",
        "print(\"Saved outputs to:\", drive_out)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "full_soil_pipeline_colab.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}