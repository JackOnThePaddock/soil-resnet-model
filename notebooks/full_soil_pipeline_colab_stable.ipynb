{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Soil Pipeline (Stable) - Colab GPU\n",
    "\n",
    "This notebook runs the full pipeline with stability guards:\n",
    "- Normalized training data + coordinates\n",
    "- GA Bare Earth extraction\n",
    "- Official pretrained SpectralGPT embeddings\n",
    "- Alpha and fused ResNet training\n",
    "- CV summary + independent validation\n",
    "- Output export to Drive\n",
    "\n",
    "Key stability fix for normalized targets:\n",
    "- `esp_consistency_weight = 0.0` (avoids NaN collapse on normalized target space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Runtime setup\n",
    "\n",
    "Set runtime to GPU in Colab before running.\n",
    "\n",
    "This notebook writes live progress to `results/metrics/colab_progress.json` and prints stage progress in each cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import subprocess\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "USE_GIT_CLONE = True\n",
    "REPO_GIT_URL = \"https://github.com/JackOnThePaddock/soil-resnet-model.git\"\n",
    "DRIVE_REPO_DIR = \"/content/drive/MyDrive/soil-resnet-model\"\n",
    "PROJECT_DIR = \"/content/soil-resnet-model\"\n",
    "\n",
    "PROGRESS_STEPS = [\n",
    "    (\"runtime_setup\", \"Runtime setup\"),\n",
    "    (\"deps_install\", \"Install dependencies\"),\n",
    "    (\"gpu_check\", \"GPU check\"),\n",
    "    (\"run_config\", \"Run config\"),\n",
    "    (\"normalized_points\", \"Prepare normalized points\"),\n",
    "    (\"bare_earth_sgpt\", \"Pull Bare Earth + SpectralGPT\"),\n",
    "    (\"fused_table\", \"Build fused feature table\"),\n",
    "    (\"write_configs\", \"Write training configs\"),\n",
    "    (\"train_alpha\", \"Train alpha model\"),\n",
    "    (\"train_fused\", \"Train fused model\"),\n",
    "    (\"checkpoint_validation\", \"Checkpoint validation\"),\n",
    "    (\"cv_summary\", \"CV summary\"),\n",
    "    (\"independent_validation\", \"Independent validation\"),\n",
    "    (\"save_outputs\", \"Save outputs\"),\n",
    "]\n",
    "PROGRESS_INDEX = {k: i + 1 for i, (k, _) in enumerate(PROGRESS_STEPS)}\n",
    "PROGRESS_LABELS = {k: v for k, v in PROGRESS_STEPS}\n",
    "# Use a bootstrap path until git clone completes, then switch to project-local progress file.\n",
    "PROGRESS_FILE = Path(\"/content/colab_progress_bootstrap.json\")\n",
    "PROGRESS_STATE = {}\n",
    "\n",
    "\n",
    "def _save_progress():\n",
    "    PROGRESS_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    payload = {\n",
    "        \"updated_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
    "        \"steps\": PROGRESS_STATE,\n",
    "    }\n",
    "    PROGRESS_FILE.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def set_progress_file(path):\n",
    "    global PROGRESS_FILE\n",
    "    PROGRESS_FILE = Path(path)\n",
    "    _save_progress()\n",
    "\n",
    "\n",
    "def progress_update(step_key, status, message=\"\"):\n",
    "    if step_key not in PROGRESS_INDEX:\n",
    "        raise KeyError(f\"Unknown progress step: {step_key}\")\n",
    "    order = PROGRESS_INDEX[step_key]\n",
    "    label = PROGRESS_LABELS[step_key]\n",
    "    PROGRESS_STATE[step_key] = {\n",
    "        \"order\": order,\n",
    "        \"label\": label,\n",
    "        \"status\": status,\n",
    "        \"message\": message,\n",
    "        \"timestamp_utc\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
    "    }\n",
    "    _save_progress()\n",
    "    done_count = sum(1 for s in PROGRESS_STATE.values() if s[\"status\"] in {\"done\", \"skipped\"})\n",
    "    pct = int((done_count / len(PROGRESS_STEPS)) * 100)\n",
    "    print(f\"[{order:02d}/{len(PROGRESS_STEPS):02d}] {label} -> {status} | overall {pct}%\")\n",
    "    if message:\n",
    "        print(\"   \", message)\n",
    "\n",
    "\n",
    "def progress_start(step_key, message=\"\"):\n",
    "    progress_update(step_key, \"running\", message)\n",
    "\n",
    "\n",
    "def progress_done(step_key, message=\"\"):\n",
    "    progress_update(step_key, \"done\", message)\n",
    "\n",
    "\n",
    "def progress_skip(step_key, message=\"\"):\n",
    "    progress_update(step_key, \"skipped\", message)\n",
    "\n",
    "\n",
    "def progress_fail(step_key, message=\"\"):\n",
    "    progress_update(step_key, \"failed\", message)\n",
    "\n",
    "\n",
    "def show_progress():\n",
    "    import pandas as _pd\n",
    "\n",
    "    rows = []\n",
    "    for key, label in PROGRESS_STEPS:\n",
    "        rec = PROGRESS_STATE.get(key, {\"status\": \"pending\", \"message\": \"\", \"timestamp_utc\": \"\"})\n",
    "        rows.append(\n",
    "            {\n",
    "                \"step\": PROGRESS_INDEX[key],\n",
    "                \"label\": label,\n",
    "                \"status\": rec[\"status\"],\n",
    "                \"message\": rec.get(\"message\", \"\"),\n",
    "                \"timestamp_utc\": rec.get(\"timestamp_utc\", \"\"),\n",
    "            }\n",
    "        )\n",
    "    display(_pd.DataFrame(rows))\n",
    "\n",
    "\n",
    "try:\n",
    "    if os.path.exists(PROJECT_DIR):\n",
    "        shutil.rmtree(PROJECT_DIR)\n",
    "\n",
    "    progress_start(\"runtime_setup\", \"Mount Drive and fetch repository\")\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "    if USE_GIT_CLONE:\n",
    "        subprocess.run([\"git\", \"clone\", REPO_GIT_URL, PROJECT_DIR], check=True)\n",
    "    else:\n",
    "        if not os.path.exists(DRIVE_REPO_DIR):\n",
    "            raise FileNotFoundError(f\"Repo not found at {DRIVE_REPO_DIR}\")\n",
    "        shutil.copytree(DRIVE_REPO_DIR, PROJECT_DIR)\n",
    "\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    set_progress_file(Path(PROJECT_DIR) / \"results/metrics/colab_progress.json\")\n",
    "    print(\"Project dir:\", os.getcwd())\n",
    "    progress_done(\"runtime_setup\", f\"Project dir set to {os.getcwd()}\")\n",
    "except Exception as e:\n",
    "    progress_fail(\"runtime_setup\", str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_start(\"deps_install\", \"Install notebook dependencies\")\n",
    "\n",
    "try:\n",
    "    commands = [\n",
    "        [\"python\", \"-V\"],\n",
    "        [\"pip\", \"install\", \"--upgrade\", \"pip\"],\n",
    "        [\"pip\", \"install\", \"-e\", \".\"],\n",
    "    ]\n",
    "    for cmd in commands:\n",
    "        print(\"Running:\", \" \".join(cmd))\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "    progress_done(\"deps_install\", \"Dependencies installed\")\n",
    "except Exception as e:\n",
    "    progress_fail(\"deps_install\", str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_start(\"gpu_check\", \"Inspect runtime accelerator\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "\n",
    "    print(\"Torch:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(\"GPU:\", gpu_name)\n",
    "        progress_done(\"gpu_check\", f\"CUDA GPU detected: {gpu_name}\")\n",
    "    else:\n",
    "        progress_done(\"gpu_check\", \"No CUDA GPU detected\")\n",
    "except Exception as e:\n",
    "    progress_fail(\"gpu_check\", str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Run config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_start(\"run_config\", \"Load run flags and paths\")\n",
    "\n",
    "try:\n",
    "    import json\n",
    "    import yaml\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    FAST_MODE = False\n",
    "    RUN_BARE_EARTH_PULL = True\n",
    "    RUN_ALPHA = True\n",
    "    RUN_FUSED = True\n",
    "    RUN_INDEPENDENT_VALIDATION = True\n",
    "\n",
    "    TRAINING = {\n",
    "        \"ensemble_size\": 3 if FAST_MODE else 5,\n",
    "        \"n_splits\": 3 if FAST_MODE else 5,\n",
    "        \"epochs_alpha\": 60 if FAST_MODE else 300,\n",
    "        \"final_epochs_alpha\": 40 if FAST_MODE else 200,\n",
    "        \"epochs_fused\": 80 if FAST_MODE else 350,\n",
    "        \"final_epochs_fused\": 60 if FAST_MODE else 220,\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 5e-5 if FAST_MODE else 8e-5,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"patience\": 20 if FAST_MODE else 35,\n",
    "        \"lr_patience\": 10 if FAST_MODE else 20,\n",
    "        \"lr_factor\": 0.5,\n",
    "        \"grad_clip\": 1.0,\n",
    "        \"random_seed\": 42,\n",
    "    }\n",
    "\n",
    "    WCS_WORKERS = 16\n",
    "    WCS_TIMEOUT = 120\n",
    "    WCS_RETRIES = 3\n",
    "    SPECTRAL_DIM = 16\n",
    "\n",
    "    paths = {\n",
    "        \"raw\": Path(\"data/processed/features.csv\"),\n",
    "        \"normalized\": Path(\"data/processed/features_normalized.csv\"),\n",
    "        \"normalized_points\": Path(\"data/processed/features_normalized_points.csv\"),\n",
    "        \"be_sgpt\": Path(\"data/processed/features_normalized_bareearth_sgpt.csv\"),\n",
    "        \"sgpt_only\": Path(\"data/processed/features_normalized_sgpt_embeddings.csv\"),\n",
    "        \"sgpt_raw\": Path(\"data/processed/features_normalized_sgpt_official_raw.csv\"),\n",
    "        \"fused_feat\": Path(\"data/processed/features_normalized_fused_feat.csv\"),\n",
    "        \"fused_meta\": Path(\"data/processed/features_normalized_fused_meta.json\"),\n",
    "        \"cfg_alpha\": Path(\"configs/colab_resnet_alpha_normalized_stable.yaml\"),\n",
    "        \"cfg_fused\": Path(\"configs/colab_resnet_fused_normalized_stable.yaml\"),\n",
    "        \"model_alpha\": Path(\"models/colab_resnet_alpha_norm_stable\"),\n",
    "        \"model_fused\": Path(\"models/colab_resnet_fused_norm_stable\"),\n",
    "        \"validation\": Path(\"data/validation/national_independent_1368.csv\"),\n",
    "    }\n",
    "\n",
    "    for req in [paths[\"raw\"], paths[\"normalized\"]]:\n",
    "        if not req.exists():\n",
    "            raise FileNotFoundError(f\"Missing required file: {req}\")\n",
    "\n",
    "    print(\"Config ready\")\n",
    "    progress_done(\"run_config\", \"Config loaded\")\n",
    "except Exception as e:\n",
    "    progress_fail(\"run_config\", str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Prepare normalized training table with coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_start(\"normalized_points\", \"Build normalized training points table\")\n",
    "\n",
    "try:\n",
    "    raw_meta = pd.read_csv(paths[\"raw\"], usecols=[\"id\", \"lat\", \"lon\"])\n",
    "    norm_df = pd.read_csv(paths[\"normalized\"])\n",
    "    if len(raw_meta) != len(norm_df):\n",
    "        raise ValueError(f\"Row mismatch raw={len(raw_meta)} normalized={len(norm_df)}\")\n",
    "\n",
    "    norm_points = pd.concat([raw_meta.reset_index(drop=True), norm_df.reset_index(drop=True)], axis=1)\n",
    "    paths[\"normalized_points\"].parent.mkdir(parents=True, exist_ok=True)\n",
    "    norm_points.to_csv(paths[\"normalized_points\"], index=False)\n",
    "    print(\"Saved\", paths[\"normalized_points\"], norm_points.shape)\n",
    "\n",
    "    progress_done(\"normalized_points\", f\"Saved {paths['normalized_points'].name} with {len(norm_points)} rows\")\n",
    "except Exception as e:\n",
    "    progress_fail(\"normalized_points\", str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Pull Bare Earth + official SpectralGPT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "progress_start(\"bare_earth_sgpt\", \"Pull Bare Earth and create official SpectralGPT embeddings\")\n",
    "\n",
    "try:\n",
    "    if RUN_BARE_EARTH_PULL:\n",
    "        cmd = [\n",
    "            \"python\", \"scripts/pull_bare_earth_embeddings.py\",\n",
    "            \"--normalized-csv\", str(paths[\"normalized\"]),\n",
    "            \"--points-csv\", str(paths[\"raw\"]),\n",
    "            \"--output-csv\", str(paths[\"be_sgpt\"]),\n",
    "            \"--output-embeddings-csv\", str(paths[\"sgpt_only\"]),\n",
    "            \"--output-official-raw-csv\", str(paths[\"sgpt_raw\"]),\n",
    "            \"--workers\", str(WCS_WORKERS),\n",
    "            \"--timeout\", str(WCS_TIMEOUT),\n",
    "            \"--retries\", str(WCS_RETRIES),\n",
    "            \"--spectral-backend\", \"official_pretrained\",\n",
    "            \"--official-request-chunk-size\", \"64\",\n",
    "            \"--spectral-dim\", str(SPECTRAL_DIM),\n",
    "            \"--seed\", str(TRAINING[\"random_seed\"]),\n",
    "        ]\n",
    "        print(\"Running:\", \" \".join(cmd))\n",
    "        subprocess.run(cmd, check=True)\n",
    "        progress_done(\"bare_earth_sgpt\", \"Bare Earth + SpectralGPT features created\")\n",
    "    else:\n",
    "        for p in [paths[\"be_sgpt\"], paths[\"sgpt_only\"]]:\n",
    "            if not p.exists():\n",
    "                raise FileNotFoundError(f\"Missing expected file: {p}\")\n",
    "        progress_skip(\"bare_earth_sgpt\", \"Using existing Bare Earth + SpectralGPT files\")\n",
    "\n",
    "    print(\"be+sgpt exists:\", paths[\"be_sgpt\"].exists())\n",
    "    print(\"sgpt exists:\", paths[\"sgpt_only\"].exists())\n",
    "except Exception as e:\n",
    "    progress_fail(\"bare_earth_sgpt\", str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Build fused feature table (`feat_*`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "progress_start(\"fused_table\", \"Build fused feat_* table\")\n",
    "\n",
    "try:\n",
    "    fused = pd.read_csv(paths[\"be_sgpt\"])\n",
    "\n",
    "    def sort_numeric_suffix(cols):\n",
    "        def key(c):\n",
    "            m = re.search(r\"(\\d+)$\", c)\n",
    "            return (0, int(m.group(1)), c) if m else (1, -1, c)\n",
    "        return sorted(cols, key=key)\n",
    "\n",
    "    band_cols = sort_numeric_suffix([c for c in fused.columns if c.lower().startswith(\"band_\")])\n",
    "    be_cols = sort_numeric_suffix([c for c in fused.columns if c.lower().startswith(\"be_\")])\n",
    "    sgpt_cols = sort_numeric_suffix([c for c in fused.columns if c.lower().startswith(\"sgpt_\")])\n",
    "\n",
    "    if len(band_cols) != 64:\n",
    "        raise ValueError(f\"Expected 64 band columns, got {len(band_cols)}\")\n",
    "    if len(sgpt_cols) == 0:\n",
    "        raise ValueError(\"No sgpt columns found\")\n",
    "\n",
    "    source_cols = band_cols + be_cols + sgpt_cols\n",
    "    for i, c in enumerate(source_cols):\n",
    "        fused[f\"feat_{i:03d}\"] = fused[c]\n",
    "\n",
    "    paths[\"fused_feat\"].parent.mkdir(parents=True, exist_ok=True)\n",
    "    fused.to_csv(paths[\"fused_feat\"], index=False)\n",
    "\n",
    "    meta = {\n",
    "        \"alpha_cols\": band_cols,\n",
    "        \"bareearth_cols\": be_cols,\n",
    "        \"spectral_cols\": sgpt_cols,\n",
    "        \"feat_cols\": [f\"feat_{i:03d}\" for i in range(len(source_cols))],\n",
    "    }\n",
    "    paths[\"fused_meta\"].write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Saved\", paths[\"fused_feat\"], fused.shape)\n",
    "    print({\"band\": len(band_cols), \"be\": len(be_cols), \"sgpt\": len(sgpt_cols), \"feat\": len(meta[\"feat_cols\"])})\n",
    "    progress_done(\"fused_table\", f\"Saved {len(meta['feat_cols'])} fused features\")\n",
    "except Exception as e:\n",
    "    progress_fail(\"fused_table\", str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Write stable training configs (normalized targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_start(\"write_configs\", \"Write stable training configs\")\n",
    "\n",
    "try:\n",
    "    # Clean old model dirs for a fresh run\n",
    "    import shutil\n",
    "    for d in [paths[\"model_alpha\"], paths[\"model_fused\"]]:\n",
    "        if d.exists():\n",
    "            shutil.rmtree(d)\n",
    "\n",
    "    TARGETS = [\"ph\", \"cec\", \"esp\", \"soc\", \"ca\", \"mg\", \"na\"]\n",
    "\n",
    "    fused_cols_df = pd.read_csv(paths[\"fused_feat\"], nrows=1)\n",
    "    n_fused_features = len([c for c in fused_cols_df.columns if c.startswith(\"feat_\")])\n",
    "\n",
    "    # IMPORTANT: esp_consistency_weight=0.0 for normalized targets\n",
    "    alpha_cfg = {\n",
    "        \"model\": {\"input_dim\": 64, \"hidden_dim\": 128, \"num_blocks\": 2, \"dropout\": 0.2, \"activation\": \"silu\"},\n",
    "        \"targets\": [\"pH\", \"CEC\", \"ESP\", \"SOC\", \"Ca\", \"Mg\", \"Na\"],\n",
    "        \"training\": {\n",
    "            \"ensemble_size\": TRAINING[\"ensemble_size\"],\n",
    "            \"n_splits\": TRAINING[\"n_splits\"],\n",
    "            \"epochs\": TRAINING[\"epochs_alpha\"],\n",
    "            \"final_epochs\": TRAINING[\"final_epochs_alpha\"],\n",
    "            \"batch_size\": TRAINING[\"batch_size\"],\n",
    "            \"learning_rate\": TRAINING[\"learning_rate\"],\n",
    "            \"weight_decay\": TRAINING[\"weight_decay\"],\n",
    "            \"patience\": TRAINING[\"patience\"],\n",
    "            \"lr_patience\": TRAINING[\"lr_patience\"],\n",
    "            \"lr_factor\": TRAINING[\"lr_factor\"],\n",
    "            \"grad_clip\": TRAINING[\"grad_clip\"],\n",
    "            \"cv_strategy\": \"group_kfold\",\n",
    "            \"loss_name\": \"weighted_huber\",\n",
    "            \"huber_delta\": 1.0,\n",
    "            \"esp_consistency_weight\": 0.0,\n",
    "            \"target_weight_mode\": \"inverse_frequency\",\n",
    "            \"sample_weight_mode\": \"rare_target_average\",\n",
    "            \"auto_target_transforms\": False,\n",
    "            \"target_transforms\": {t: \"identity\" for t in TARGETS},\n",
    "            \"specialist_targets\": [\"cec\", \"esp\", \"soc\"],\n",
    "            \"specialist_epochs\": 60 if FAST_MODE else 120,\n",
    "            \"specialist_patience\": 15 if FAST_MODE else 20,\n",
    "            \"specialist_val_fraction\": 0.2,\n",
    "            \"specialist_blend_weight\": 0.4,\n",
    "            \"random_seed\": TRAINING[\"random_seed\"],\n",
    "        },\n",
    "        \"data\": {\"feature_prefix\": \"band_\", \"n_features\": 64, \"group_by\": \"latlon\", \"group_round\": 4, \"reference_data\": None},\n",
    "        \"output\": {\"model_dir\": str(paths[\"model_alpha\"]), \"metrics_dir\": \"results/metrics\", \"scaler_file\": \"scaler.pkl\"},\n",
    "    }\n",
    "\n",
    "    fused_cfg = {\n",
    "        \"model\": {\"input_dim\": n_fused_features, \"hidden_dim\": 192, \"num_blocks\": 3, \"dropout\": 0.2, \"activation\": \"silu\"},\n",
    "        \"targets\": [\"pH\", \"CEC\", \"ESP\", \"SOC\", \"Ca\", \"Mg\", \"Na\"],\n",
    "        \"training\": {\n",
    "            \"ensemble_size\": TRAINING[\"ensemble_size\"],\n",
    "            \"n_splits\": TRAINING[\"n_splits\"],\n",
    "            \"epochs\": TRAINING[\"epochs_fused\"],\n",
    "            \"final_epochs\": TRAINING[\"final_epochs_fused\"],\n",
    "            \"batch_size\": TRAINING[\"batch_size\"],\n",
    "            \"learning_rate\": TRAINING[\"learning_rate\"],\n",
    "            \"weight_decay\": TRAINING[\"weight_decay\"],\n",
    "            \"patience\": TRAINING[\"patience\"],\n",
    "            \"lr_patience\": TRAINING[\"lr_patience\"],\n",
    "            \"lr_factor\": TRAINING[\"lr_factor\"],\n",
    "            \"grad_clip\": TRAINING[\"grad_clip\"],\n",
    "            \"cv_strategy\": \"group_kfold\",\n",
    "            \"loss_name\": \"weighted_huber\",\n",
    "            \"huber_delta\": 1.0,\n",
    "            \"esp_consistency_weight\": 0.0,\n",
    "            \"target_weight_mode\": \"inverse_frequency\",\n",
    "            \"sample_weight_mode\": \"rare_target_average\",\n",
    "            \"auto_target_transforms\": False,\n",
    "            \"target_transforms\": {t: \"identity\" for t in TARGETS},\n",
    "            \"specialist_targets\": [\"cec\", \"esp\", \"soc\"],\n",
    "            \"specialist_epochs\": 80 if FAST_MODE else 150,\n",
    "            \"specialist_patience\": 15 if FAST_MODE else 25,\n",
    "            \"specialist_val_fraction\": 0.2,\n",
    "            \"specialist_blend_weight\": 0.4,\n",
    "            \"random_seed\": TRAINING[\"random_seed\"],\n",
    "        },\n",
    "        \"data\": {\"feature_prefix\": \"feat_\", \"n_features\": None, \"group_by\": \"latlon\", \"group_round\": 4, \"reference_data\": None},\n",
    "        \"output\": {\"model_dir\": str(paths[\"model_fused\"]), \"metrics_dir\": \"results/metrics\", \"scaler_file\": \"scaler.pkl\"},\n",
    "    }\n",
    "\n",
    "    paths[\"cfg_alpha\"].write_text(yaml.safe_dump(alpha_cfg, sort_keys=False), encoding=\"utf-8\")\n",
    "    paths[\"cfg_fused\"].write_text(yaml.safe_dump(fused_cfg, sort_keys=False), encoding=\"utf-8\")\n",
    "    print(\"Wrote\", paths[\"cfg_alpha\"])\n",
    "    print(\"Wrote\", paths[\"cfg_fused\"])\n",
    "\n",
    "    progress_done(\"write_configs\", \"Training configs written\")\n",
    "except Exception as e:\n",
    "    progress_fail(\"write_configs\", str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Train alpha model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "progress_start(\"train_alpha\", \"Train alpha model ensemble\")\n",
    "\n",
    "try:\n",
    "    if RUN_ALPHA:\n",
    "        cmd = [\n",
    "            \"python\", \"scripts/train_resnet_ensemble.py\",\n",
    "            \"--data\", str(paths[\"normalized_points\"]),\n",
    "            \"--config\", str(paths[\"cfg_alpha\"]),\n",
    "            \"--output\", str(paths[\"model_alpha\"]),\n",
    "            \"--cv-strategy\", \"group_kfold\",\n",
    "            \"--seed\", str(TRAINING[\"random_seed\"]),\n",
    "        ]\n",
    "        print(\"Running:\", \" \".join(cmd))\n",
    "        subprocess.run(cmd, check=True)\n",
    "        progress_done(\"train_alpha\", \"Alpha ensemble training finished\")\n",
    "    else:\n",
    "        print(\"Skipped alpha training\")\n",
    "        progress_skip(\"train_alpha\", \"RUN_ALPHA=False\")\n",
    "except Exception as e:\n",
    "    progress_fail(\"train_alpha\", str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Train fused model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_start(\"train_fused\", \"Train fused model ensemble\")\n",
    "\n",
    "try:\n",
    "    if RUN_FUSED:\n",
    "        cmd = [\n",
    "            \"python\", \"scripts/train_resnet_ensemble.py\",\n",
    "            \"--data\", str(paths[\"fused_feat\"]),\n",
    "            \"--config\", str(paths[\"cfg_fused\"]),\n",
    "            \"--output\", str(paths[\"model_fused\"]),\n",
    "            \"--cv-strategy\", \"group_kfold\",\n",
    "            \"--seed\", str(TRAINING[\"random_seed\"]),\n",
    "        ]\n",
    "        print(\"Running:\", \" \".join(cmd))\n",
    "        subprocess.run(cmd, check=True)\n",
    "        progress_done(\"train_fused\", \"Fused ensemble training finished\")\n",
    "    else:\n",
    "        print(\"Skipped fused training\")\n",
    "        progress_skip(\"train_fused\", \"RUN_FUSED=False\")\n",
    "except Exception as e:\n",
    "    progress_fail(\"train_fused\", str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Check model checkpoints for NaN/Inf (fail fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "progress_start(\"checkpoint_validation\", \"Validate checkpoint tensors for NaN/Inf\")\n",
    "\n",
    "try:\n",
    "    def check_model_dir(model_dir: Path):\n",
    "        model_files = sorted(model_dir.glob(\"model_*.pth\"))\n",
    "        if not model_files:\n",
    "            raise FileNotFoundError(f\"No model_*.pth found in {model_dir}\")\n",
    "\n",
    "        print(\"Checking\", model_dir)\n",
    "        for mf in model_files:\n",
    "            ckpt = torch.load(mf, map_location=\"cpu\", weights_only=False)\n",
    "            state = ckpt[\"model_state_dict\"]\n",
    "            bad = []\n",
    "            for k, v in state.items():\n",
    "                if torch.is_tensor(v) and (torch.isnan(v).any() or torch.isinf(v).any()):\n",
    "                    bad.append(k)\n",
    "            if bad:\n",
    "                raise RuntimeError(f\"Invalid weights in {mf.name}: {bad[:5]}\")\n",
    "            print(\"  OK\", mf.name)\n",
    "\n",
    "    if RUN_ALPHA:\n",
    "        check_model_dir(paths[\"model_alpha\"])\n",
    "    if RUN_FUSED:\n",
    "        check_model_dir(paths[\"model_fused\"])\n",
    "    print(\"All checkpoints finite\")\n",
    "    progress_done(\"checkpoint_validation\", \"All checkpoints finite\")\n",
    "except Exception as e:\n",
    "    progress_fail(\"checkpoint_validation\", str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) CV summary from `ensemble_metrics.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "progress_start(\"cv_summary\", \"Summarize cross-validation metrics\")\n",
    "\n",
    "try:\n",
    "    def summarize_ensemble_metrics(csv_path: Path, model_label: str) -> pd.DataFrame:\n",
    "        if not csv_path.exists():\n",
    "            return pd.DataFrame([{\"model\": model_label, \"target\": \"(missing)\", \"metric\": \"\", \"mean\": np.nan, \"std\": np.nan}])\n",
    "        df = pd.read_csv(csv_path)\n",
    "        rows = []\n",
    "        for col in df.columns:\n",
    "            m = re.match(r\"^(.*)_(r2|rmse|mae)$\", col)\n",
    "            if m:\n",
    "                rows.append({\n",
    "                    \"model\": model_label,\n",
    "                    \"target\": m.group(1).lower(),\n",
    "                    \"metric\": m.group(2),\n",
    "                    \"mean\": float(df[col].mean()),\n",
    "                    \"std\": float(df[col].std(ddof=0)),\n",
    "                })\n",
    "        if not rows:\n",
    "            rows.append({\"model\": model_label, \"target\": \"(no per-target metrics)\", \"metric\": \"\", \"mean\": np.nan, \"std\": np.nan})\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    alpha_cv = summarize_ensemble_metrics(paths[\"model_alpha\"] / \"ensemble_metrics.csv\", \"alpha_norm_stable\")\n",
    "    fused_cv = summarize_ensemble_metrics(paths[\"model_fused\"] / \"ensemble_metrics.csv\", \"fused_norm_stable\")\n",
    "    cv_summary = pd.concat([alpha_cv, fused_cv], ignore_index=True)\n",
    "\n",
    "    display(cv_summary)\n",
    "\n",
    "    cv_out = Path(\"results/metrics/colab_cv_summary_stable.csv\")\n",
    "    cv_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cv_summary.to_csv(cv_out, index=False)\n",
    "    print(\"Saved\", cv_out)\n",
    "    progress_done(\"cv_summary\", \"Saved CV summary CSV\")\n",
    "except Exception as e:\n",
    "    progress_fail(\"cv_summary\", str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Independent validation (alpha model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_start(\"independent_validation\", \"Run independent validation set\")\n",
    "\n",
    "try:\n",
    "    if RUN_INDEPENDENT_VALIDATION:\n",
    "        from src.models.ensemble import SoilEnsemble\n",
    "        from src.evaluation.metrics import compute_metrics\n",
    "\n",
    "        val_path = paths[\"validation\"]\n",
    "        if not val_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing validation file: {val_path}\")\n",
    "\n",
    "        val_df = pd.read_csv(val_path)\n",
    "        for i in range(64):\n",
    "            src = f\"A{i:02d}\"\n",
    "            if src in val_df.columns:\n",
    "                val_df[f\"band_{i}\"] = val_df[src]\n",
    "\n",
    "        feature_cols = [f\"band_{i}\" for i in range(64)]\n",
    "        missing = [c for c in feature_cols if c not in val_df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Validation missing feature columns: {missing[:5]}\")\n",
    "\n",
    "        model = SoilEnsemble(paths[\"model_alpha\"])\n",
    "        pred_norm, _ = model.predict_batch(val_df[feature_cols].values.astype(np.float32))\n",
    "\n",
    "        targets = [\"ph\", \"cec\", \"esp\", \"soc\", \"ca\", \"mg\", \"na\"]\n",
    "        train_raw = pd.read_csv(paths[\"raw\"], usecols=targets)\n",
    "        mu = train_raw.mean(axis=0)\n",
    "        sigma = train_raw.std(axis=0, ddof=0).replace(0.0, 1.0)\n",
    "\n",
    "        pred_norm_df = pd.DataFrame(pred_norm, columns=model.target_names)\n",
    "        pred_raw_df = pred_norm_df.copy()\n",
    "        for t in model.target_names:\n",
    "            pred_raw_df[t] = pred_norm_df[t] * float(sigma[t]) + float(mu[t])\n",
    "\n",
    "        y_true_raw = pd.DataFrame(np.nan, index=val_df.index, columns=model.target_names)\n",
    "        truth_map = {\"ph\": \"ph\", \"cec\": \"cec_cmolkg\", \"esp\": \"esp_pct\", \"na\": \"na_cmolkg\"}\n",
    "        for t, src_col in truth_map.items():\n",
    "            if src_col in val_df.columns:\n",
    "                y_true_raw[t] = pd.to_numeric(val_df[src_col], errors=\"coerce\")\n",
    "\n",
    "        metrics = compute_metrics(\n",
    "            y_true=y_true_raw[model.target_names].values,\n",
    "            y_pred=pred_raw_df[model.target_names].values,\n",
    "            target_names=model.target_names,\n",
    "        )\n",
    "\n",
    "        indep_df = pd.DataFrame(metrics).T.reset_index().rename(columns={\"index\": \"target\"}) if metrics else pd.DataFrame()\n",
    "        display(indep_df)\n",
    "\n",
    "        indep_out = Path(\"results/metrics/colab_independent_validation_alpha_stable.csv\")\n",
    "        indep_df.to_csv(indep_out, index=False)\n",
    "        print(\"Saved\", indep_out)\n",
    "        progress_done(\"independent_validation\", \"Saved independent validation CSV\")\n",
    "    else:\n",
    "        print(\"Skipped independent validation\")\n",
    "        progress_skip(\"independent_validation\", \"RUN_INDEPENDENT_VALIDATION=False\")\n",
    "except Exception as e:\n",
    "    progress_fail(\"independent_validation\", str(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Save outputs to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "progress_start(\"save_outputs\", \"Copy outputs to Google Drive\")\n",
    "\n",
    "try:\n",
    "    stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    drive_out = Path(f\"/content/drive/MyDrive/soil-resnet-outputs/full_pipeline_stable_{stamp}\")\n",
    "    drive_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    copy_targets = [\n",
    "        paths[\"normalized_points\"],\n",
    "        paths[\"be_sgpt\"],\n",
    "        paths[\"sgpt_only\"],\n",
    "        paths[\"sgpt_raw\"],\n",
    "        paths[\"fused_feat\"],\n",
    "        paths[\"fused_meta\"],\n",
    "        paths[\"cfg_alpha\"],\n",
    "        paths[\"cfg_fused\"],\n",
    "        Path(\"results/metrics/colab_cv_summary_stable.csv\"),\n",
    "        Path(\"results/metrics/colab_independent_validation_alpha_stable.csv\"),\n",
    "        PROGRESS_FILE,\n",
    "    ]\n",
    "\n",
    "    for p in copy_targets:\n",
    "        if p.exists():\n",
    "            shutil.copy2(p, drive_out / p.name)\n",
    "\n",
    "    if paths[\"model_alpha\"].exists():\n",
    "        shutil.copytree(paths[\"model_alpha\"], drive_out / paths[\"model_alpha\"].name)\n",
    "    if paths[\"model_fused\"].exists():\n",
    "        shutil.copytree(paths[\"model_fused\"], drive_out / paths[\"model_fused\"].name)\n",
    "\n",
    "    print(\"Saved outputs to:\", drive_out)\n",
    "    progress_done(\"save_outputs\", f\"Outputs copied to {drive_out}\")\n",
    "\n",
    "    print(\"\\nProgress snapshot:\")\n",
    "    show_progress()\n",
    "except Exception as e:\n",
    "    progress_fail(\"save_outputs\", str(e))\n",
    "    raise\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "full_soil_pipeline_colab_stable.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
