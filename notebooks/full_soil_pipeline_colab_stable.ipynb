{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full Soil Pipeline (Stable) - Colab GPU\n",
        "\n",
        "This notebook runs the full pipeline with stability guards:\n",
        "- Normalized training data + coordinates\n",
        "- GA Bare Earth extraction\n",
        "- Official pretrained SpectralGPT embeddings\n",
        "- Alpha and fused ResNet training\n",
        "- CV summary + independent validation\n",
        "- Output export to Drive\n",
        "\n",
        "Key stability fix for normalized targets:\n",
        "- `esp_consistency_weight = 0.0` (avoids NaN collapse on normalized target space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Runtime setup\n",
        "\n",
        "Set runtime to GPU in Colab before running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "USE_GIT_CLONE = True\n",
        "REPO_GIT_URL = \"https://github.com/JackOnThePaddock/soil-resnet-model.git\"\n",
        "DRIVE_REPO_DIR = \"/content/drive/MyDrive/soil-resnet-model\"\n",
        "PROJECT_DIR = \"/content/soil-resnet-model\"\n",
        "\n",
        "if os.path.exists(PROJECT_DIR):\n",
        "    shutil.rmtree(PROJECT_DIR)\n",
        "\n",
        "if USE_GIT_CLONE:\n",
        "    subprocess.run([\"git\", \"clone\", REPO_GIT_URL, PROJECT_DIR], check=True)\n",
        "else:\n",
        "    if not os.path.exists(DRIVE_REPO_DIR):\n",
        "        raise FileNotFoundError(f\"Repo not found at {DRIVE_REPO_DIR}\")\n",
        "    shutil.copytree(DRIVE_REPO_DIR, PROJECT_DIR)\n",
        "\n",
        "os.chdir(PROJECT_DIR)\n",
        "print(\"Project dir:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -V\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Run config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "FAST_MODE = False\n",
        "RUN_BARE_EARTH_PULL = True\n",
        "RUN_ALPHA = True\n",
        "RUN_FUSED = True\n",
        "RUN_INDEPENDENT_VALIDATION = True\n",
        "\n",
        "TRAINING = {\n",
        "    \"ensemble_size\": 3 if FAST_MODE else 5,\n",
        "    \"n_splits\": 3 if FAST_MODE else 5,\n",
        "    \"epochs_alpha\": 60 if FAST_MODE else 300,\n",
        "    \"final_epochs_alpha\": 40 if FAST_MODE else 200,\n",
        "    \"epochs_fused\": 80 if FAST_MODE else 350,\n",
        "    \"final_epochs_fused\": 60 if FAST_MODE else 220,\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 5e-5 if FAST_MODE else 8e-5,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"patience\": 20 if FAST_MODE else 35,\n",
        "    \"lr_patience\": 10 if FAST_MODE else 20,\n",
        "    \"lr_factor\": 0.5,\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"random_seed\": 42,\n",
        "}\n",
        "\n",
        "WCS_WORKERS = 16\n",
        "WCS_TIMEOUT = 120\n",
        "WCS_RETRIES = 3\n",
        "SPECTRAL_DIM = 16\n",
        "\n",
        "paths = {\n",
        "    \"raw\": Path(\"data/processed/features.csv\"),\n",
        "    \"normalized\": Path(\"data/processed/features_normalized.csv\"),\n",
        "    \"normalized_points\": Path(\"data/processed/features_normalized_points.csv\"),\n",
        "    \"be_sgpt\": Path(\"data/processed/features_normalized_bareearth_sgpt.csv\"),\n",
        "    \"sgpt_only\": Path(\"data/processed/features_normalized_sgpt_embeddings.csv\"),\n",
        "    \"sgpt_raw\": Path(\"data/processed/features_normalized_sgpt_official_raw.csv\"),\n",
        "    \"fused_feat\": Path(\"data/processed/features_normalized_fused_feat.csv\"),\n",
        "    \"fused_meta\": Path(\"data/processed/features_normalized_fused_meta.json\"),\n",
        "    \"cfg_alpha\": Path(\"configs/colab_resnet_alpha_normalized_stable.yaml\"),\n",
        "    \"cfg_fused\": Path(\"configs/colab_resnet_fused_normalized_stable.yaml\"),\n",
        "    \"model_alpha\": Path(\"models/colab_resnet_alpha_norm_stable\"),\n",
        "    \"model_fused\": Path(\"models/colab_resnet_fused_norm_stable\"),\n",
        "    \"validation\": Path(\"data/validation/national_independent_1368.csv\"),\n",
        "}\n",
        "\n",
        "for req in [paths[\"raw\"], paths[\"normalized\"]]:\n",
        "    if not req.exists():\n",
        "        raise FileNotFoundError(f\"Missing required file: {req}\")\n",
        "\n",
        "print(\"Config ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Prepare normalized training table with coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_meta = pd.read_csv(paths[\"raw\"], usecols=[\"id\", \"lat\", \"lon\"])\n",
        "norm_df = pd.read_csv(paths[\"normalized\"])\n",
        "if len(raw_meta) != len(norm_df):\n",
        "    raise ValueError(f\"Row mismatch raw={len(raw_meta)} normalized={len(norm_df)}\")\n",
        "\n",
        "norm_points = pd.concat([raw_meta.reset_index(drop=True), norm_df.reset_index(drop=True)], axis=1)\n",
        "paths[\"normalized_points\"].parent.mkdir(parents=True, exist_ok=True)\n",
        "norm_points.to_csv(paths[\"normalized_points\"], index=False)\n",
        "print(\"Saved\", paths[\"normalized_points\"], norm_points.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Pull Bare Earth + official SpectralGPT embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "if RUN_BARE_EARTH_PULL:\n",
        "    cmd = [\n",
        "        \"python\", \"scripts/pull_bare_earth_embeddings.py\",\n",
        "        \"--normalized-csv\", str(paths[\"normalized\"]),\n",
        "        \"--points-csv\", str(paths[\"raw\"]),\n",
        "        \"--output-csv\", str(paths[\"be_sgpt\"]),\n",
        "        \"--output-embeddings-csv\", str(paths[\"sgpt_only\"]),\n",
        "        \"--output-official-raw-csv\", str(paths[\"sgpt_raw\"]),\n",
        "        \"--workers\", str(WCS_WORKERS),\n",
        "        \"--timeout\", str(WCS_TIMEOUT),\n",
        "        \"--retries\", str(WCS_RETRIES),\n",
        "        \"--spectral-backend\", \"official_pretrained\",\n",
        "        \"--official-request-chunk-size\", \"64\",\n",
        "        \"--spectral-dim\", str(SPECTRAL_DIM),\n",
        "        \"--seed\", str(TRAINING[\"random_seed\"]),\n",
        "    ]\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "else:\n",
        "    for p in [paths[\"be_sgpt\"], paths[\"sgpt_only\"]]:\n",
        "        if not p.exists():\n",
        "            raise FileNotFoundError(f\"Missing expected file: {p}\")\n",
        "\n",
        "print(\"be+sgpt exists:\", paths[\"be_sgpt\"].exists())\n",
        "print(\"sgpt exists:\", paths[\"sgpt_only\"].exists())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Build fused feature table (`feat_*`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "fused = pd.read_csv(paths[\"be_sgpt\"])\n",
        "\n",
        "def sort_numeric_suffix(cols):\n",
        "    def key(c):\n",
        "        m = re.search(r\"(\\d+)$\", c)\n",
        "        return (0, int(m.group(1)), c) if m else (1, -1, c)\n",
        "    return sorted(cols, key=key)\n",
        "\n",
        "band_cols = sort_numeric_suffix([c for c in fused.columns if c.lower().startswith(\"band_\")])\n",
        "be_cols = sort_numeric_suffix([c for c in fused.columns if c.lower().startswith(\"be_\")])\n",
        "sgpt_cols = sort_numeric_suffix([c for c in fused.columns if c.lower().startswith(\"sgpt_\")])\n",
        "\n",
        "if len(band_cols) != 64:\n",
        "    raise ValueError(f\"Expected 64 band columns, got {len(band_cols)}\")\n",
        "if len(sgpt_cols) == 0:\n",
        "    raise ValueError(\"No sgpt columns found\")\n",
        "\n",
        "source_cols = band_cols + be_cols + sgpt_cols\n",
        "for i, c in enumerate(source_cols):\n",
        "    fused[f\"feat_{i:03d}\"] = fused[c]\n",
        "\n",
        "paths[\"fused_feat\"].parent.mkdir(parents=True, exist_ok=True)\n",
        "fused.to_csv(paths[\"fused_feat\"], index=False)\n",
        "\n",
        "meta = {\n",
        "    \"alpha_cols\": band_cols,\n",
        "    \"bareearth_cols\": be_cols,\n",
        "    \"spectral_cols\": sgpt_cols,\n",
        "    \"feat_cols\": [f\"feat_{i:03d}\" for i in range(len(source_cols))],\n",
        "}\n",
        "paths[\"fused_meta\"].write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"Saved\", paths[\"fused_feat\"], fused.shape)\n",
        "print({\"band\": len(band_cols), \"be\": len(be_cols), \"sgpt\": len(sgpt_cols), \"feat\": len(meta[\"feat_cols\"])})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Write stable training configs (normalized targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean old model dirs for a fresh run\n",
        "import shutil\n",
        "for d in [paths[\"model_alpha\"], paths[\"model_fused\"]]:\n",
        "    if d.exists():\n",
        "        shutil.rmtree(d)\n",
        "\n",
        "TARGETS = [\"ph\", \"cec\", \"esp\", \"soc\", \"ca\", \"mg\", \"na\"]\n",
        "\n",
        "fused_cols_df = pd.read_csv(paths[\"fused_feat\"], nrows=1)\n",
        "n_fused_features = len([c for c in fused_cols_df.columns if c.startswith(\"feat_\")])\n",
        "\n",
        "# IMPORTANT: esp_consistency_weight=0.0 for normalized targets\n",
        "alpha_cfg = {\n",
        "    \"model\": {\"input_dim\": 64, \"hidden_dim\": 128, \"num_blocks\": 2, \"dropout\": 0.2, \"activation\": \"silu\"},\n",
        "    \"targets\": [\"pH\", \"CEC\", \"ESP\", \"SOC\", \"Ca\", \"Mg\", \"Na\"],\n",
        "    \"training\": {\n",
        "        \"ensemble_size\": TRAINING[\"ensemble_size\"],\n",
        "        \"n_splits\": TRAINING[\"n_splits\"],\n",
        "        \"epochs\": TRAINING[\"epochs_alpha\"],\n",
        "        \"final_epochs\": TRAINING[\"final_epochs_alpha\"],\n",
        "        \"batch_size\": TRAINING[\"batch_size\"],\n",
        "        \"learning_rate\": TRAINING[\"learning_rate\"],\n",
        "        \"weight_decay\": TRAINING[\"weight_decay\"],\n",
        "        \"patience\": TRAINING[\"patience\"],\n",
        "        \"lr_patience\": TRAINING[\"lr_patience\"],\n",
        "        \"lr_factor\": TRAINING[\"lr_factor\"],\n",
        "        \"grad_clip\": TRAINING[\"grad_clip\"],\n",
        "        \"cv_strategy\": \"group_kfold\",\n",
        "        \"loss_name\": \"weighted_huber\",\n",
        "        \"huber_delta\": 1.0,\n",
        "        \"esp_consistency_weight\": 0.0,\n",
        "        \"target_weight_mode\": \"inverse_frequency\",\n",
        "        \"sample_weight_mode\": \"rare_target_average\",\n",
        "        \"auto_target_transforms\": False,\n",
        "        \"target_transforms\": {t: \"identity\" for t in TARGETS},\n",
        "        \"specialist_targets\": [\"cec\", \"esp\", \"soc\"],\n",
        "        \"specialist_epochs\": 60 if FAST_MODE else 120,\n",
        "        \"specialist_patience\": 15 if FAST_MODE else 20,\n",
        "        \"specialist_val_fraction\": 0.2,\n",
        "        \"specialist_blend_weight\": 0.4,\n",
        "        \"random_seed\": TRAINING[\"random_seed\"],\n",
        "    },\n",
        "    \"data\": {\"feature_prefix\": \"band_\", \"n_features\": 64, \"group_by\": \"latlon\", \"group_round\": 4, \"reference_data\": None},\n",
        "    \"output\": {\"model_dir\": str(paths[\"model_alpha\"]), \"metrics_dir\": \"results/metrics\", \"scaler_file\": \"scaler.pkl\"},\n",
        "}\n",
        "\n",
        "fused_cfg = {\n",
        "    \"model\": {\"input_dim\": n_fused_features, \"hidden_dim\": 192, \"num_blocks\": 3, \"dropout\": 0.2, \"activation\": \"silu\"},\n",
        "    \"targets\": [\"pH\", \"CEC\", \"ESP\", \"SOC\", \"Ca\", \"Mg\", \"Na\"],\n",
        "    \"training\": {\n",
        "        \"ensemble_size\": TRAINING[\"ensemble_size\"],\n",
        "        \"n_splits\": TRAINING[\"n_splits\"],\n",
        "        \"epochs\": TRAINING[\"epochs_fused\"],\n",
        "        \"final_epochs\": TRAINING[\"final_epochs_fused\"],\n",
        "        \"batch_size\": TRAINING[\"batch_size\"],\n",
        "        \"learning_rate\": TRAINING[\"learning_rate\"],\n",
        "        \"weight_decay\": TRAINING[\"weight_decay\"],\n",
        "        \"patience\": TRAINING[\"patience\"],\n",
        "        \"lr_patience\": TRAINING[\"lr_patience\"],\n",
        "        \"lr_factor\": TRAINING[\"lr_factor\"],\n",
        "        \"grad_clip\": TRAINING[\"grad_clip\"],\n",
        "        \"cv_strategy\": \"group_kfold\",\n",
        "        \"loss_name\": \"weighted_huber\",\n",
        "        \"huber_delta\": 1.0,\n",
        "        \"esp_consistency_weight\": 0.0,\n",
        "        \"target_weight_mode\": \"inverse_frequency\",\n",
        "        \"sample_weight_mode\": \"rare_target_average\",\n",
        "        \"auto_target_transforms\": False,\n",
        "        \"target_transforms\": {t: \"identity\" for t in TARGETS},\n",
        "        \"specialist_targets\": [\"cec\", \"esp\", \"soc\"],\n",
        "        \"specialist_epochs\": 80 if FAST_MODE else 150,\n",
        "        \"specialist_patience\": 15 if FAST_MODE else 25,\n",
        "        \"specialist_val_fraction\": 0.2,\n",
        "        \"specialist_blend_weight\": 0.4,\n",
        "        \"random_seed\": TRAINING[\"random_seed\"],\n",
        "    },\n",
        "    \"data\": {\"feature_prefix\": \"feat_\", \"n_features\": None, \"group_by\": \"latlon\", \"group_round\": 4, \"reference_data\": None},\n",
        "    \"output\": {\"model_dir\": str(paths[\"model_fused\"]), \"metrics_dir\": \"results/metrics\", \"scaler_file\": \"scaler.pkl\"},\n",
        "}\n",
        "\n",
        "paths[\"cfg_alpha\"].write_text(yaml.safe_dump(alpha_cfg, sort_keys=False), encoding=\"utf-8\")\n",
        "paths[\"cfg_fused\"].write_text(yaml.safe_dump(fused_cfg, sort_keys=False), encoding=\"utf-8\")\n",
        "print(\"Wrote\", paths[\"cfg_alpha\"])\n",
        "print(\"Wrote\", paths[\"cfg_fused\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Train alpha model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "if RUN_ALPHA:\n",
        "    cmd = [\n",
        "        \"python\", \"scripts/train_resnet_ensemble.py\",\n",
        "        \"--data\", str(paths[\"normalized_points\"]),\n",
        "        \"--config\", str(paths[\"cfg_alpha\"]),\n",
        "        \"--output\", str(paths[\"model_alpha\"]),\n",
        "        \"--cv-strategy\", \"group_kfold\",\n",
        "        \"--seed\", str(TRAINING[\"random_seed\"]),\n",
        "    ]\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "else:\n",
        "    print(\"Skipped alpha training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Train fused model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_FUSED:\n",
        "    cmd = [\n",
        "        \"python\", \"scripts/train_resnet_ensemble.py\",\n",
        "        \"--data\", str(paths[\"fused_feat\"]),\n",
        "        \"--config\", str(paths[\"cfg_fused\"]),\n",
        "        \"--output\", str(paths[\"model_fused\"]),\n",
        "        \"--cv-strategy\", \"group_kfold\",\n",
        "        \"--seed\", str(TRAINING[\"random_seed\"]),\n",
        "    ]\n",
        "    print(\"Running:\", \" \".join(cmd))\n",
        "    subprocess.run(cmd, check=True)\n",
        "else:\n",
        "    print(\"Skipped fused training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Check model checkpoints for NaN/Inf (fail fast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def check_model_dir(model_dir: Path):\n",
        "    model_files = sorted(model_dir.glob(\"model_*.pth\"))\n",
        "    if not model_files:\n",
        "        raise FileNotFoundError(f\"No model_*.pth found in {model_dir}\")\n",
        "\n",
        "    print(\"Checking\", model_dir)\n",
        "    for mf in model_files:\n",
        "        ckpt = torch.load(mf, map_location=\"cpu\", weights_only=False)\n",
        "        state = ckpt[\"model_state_dict\"]\n",
        "        bad = []\n",
        "        for k, v in state.items():\n",
        "            if torch.is_tensor(v) and (torch.isnan(v).any() or torch.isinf(v).any()):\n",
        "                bad.append(k)\n",
        "        if bad:\n",
        "            raise RuntimeError(f\"Invalid weights in {mf.name}: {bad[:5]}\")\n",
        "        print(\"  OK\", mf.name)\n",
        "\n",
        "if RUN_ALPHA:\n",
        "    check_model_dir(paths[\"model_alpha\"])\n",
        "if RUN_FUSED:\n",
        "    check_model_dir(paths[\"model_fused\"])\n",
        "print(\"All checkpoints finite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) CV summary from `ensemble_metrics.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def summarize_ensemble_metrics(csv_path: Path, model_label: str) -> pd.DataFrame:\n",
        "    if not csv_path.exists():\n",
        "        return pd.DataFrame([{\"model\": model_label, \"target\": \"(missing)\", \"metric\": \"\", \"mean\": np.nan, \"std\": np.nan}])\n",
        "    df = pd.read_csv(csv_path)\n",
        "    rows = []\n",
        "    for col in df.columns:\n",
        "        m = re.match(r\"^(.*)_(r2|rmse|mae)$\", col)\n",
        "        if m:\n",
        "            rows.append({\n",
        "                \"model\": model_label,\n",
        "                \"target\": m.group(1).lower(),\n",
        "                \"metric\": m.group(2),\n",
        "                \"mean\": float(df[col].mean()),\n",
        "                \"std\": float(df[col].std(ddof=0)),\n",
        "            })\n",
        "    if not rows:\n",
        "        rows.append({\"model\": model_label, \"target\": \"(no per-target metrics)\", \"metric\": \"\", \"mean\": np.nan, \"std\": np.nan})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "alpha_cv = summarize_ensemble_metrics(paths[\"model_alpha\"] / \"ensemble_metrics.csv\", \"alpha_norm_stable\")\n",
        "fused_cv = summarize_ensemble_metrics(paths[\"model_fused\"] / \"ensemble_metrics.csv\", \"fused_norm_stable\")\n",
        "cv_summary = pd.concat([alpha_cv, fused_cv], ignore_index=True)\n",
        "\n",
        "display(cv_summary)\n",
        "\n",
        "cv_out = Path(\"results/metrics/colab_cv_summary_stable.csv\")\n",
        "cv_out.parent.mkdir(parents=True, exist_ok=True)\n",
        "cv_summary.to_csv(cv_out, index=False)\n",
        "print(\"Saved\", cv_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Independent validation (alpha model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_INDEPENDENT_VALIDATION:\n",
        "    from src.models.ensemble import SoilEnsemble\n",
        "    from src.evaluation.metrics import compute_metrics\n",
        "\n",
        "    val_path = paths[\"validation\"]\n",
        "    if not val_path.exists():\n",
        "        raise FileNotFoundError(f\"Missing validation file: {val_path}\")\n",
        "\n",
        "    val_df = pd.read_csv(val_path)\n",
        "    for i in range(64):\n",
        "        src = f\"A{i:02d}\"\n",
        "        if src in val_df.columns:\n",
        "            val_df[f\"band_{i}\"] = val_df[src]\n",
        "\n",
        "    feature_cols = [f\"band_{i}\" for i in range(64)]\n",
        "    missing = [c for c in feature_cols if c not in val_df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Validation missing feature columns: {missing[:5]}\")\n",
        "\n",
        "    model = SoilEnsemble(paths[\"model_alpha\"])\n",
        "    pred_norm, _ = model.predict_batch(val_df[feature_cols].values.astype(np.float32))\n",
        "\n",
        "    targets = [\"ph\", \"cec\", \"esp\", \"soc\", \"ca\", \"mg\", \"na\"]\n",
        "    train_raw = pd.read_csv(paths[\"raw\"], usecols=targets)\n",
        "    mu = train_raw.mean(axis=0)\n",
        "    sigma = train_raw.std(axis=0, ddof=0).replace(0.0, 1.0)\n",
        "\n",
        "    pred_norm_df = pd.DataFrame(pred_norm, columns=model.target_names)\n",
        "    pred_raw_df = pred_norm_df.copy()\n",
        "    for t in model.target_names:\n",
        "        pred_raw_df[t] = pred_norm_df[t] * float(sigma[t]) + float(mu[t])\n",
        "\n",
        "    y_true_raw = pd.DataFrame(np.nan, index=val_df.index, columns=model.target_names)\n",
        "    truth_map = {\"ph\": \"ph\", \"cec\": \"cec_cmolkg\", \"esp\": \"esp_pct\", \"na\": \"na_cmolkg\"}\n",
        "    for t, src_col in truth_map.items():\n",
        "        if src_col in val_df.columns:\n",
        "            y_true_raw[t] = pd.to_numeric(val_df[src_col], errors=\"coerce\")\n",
        "\n",
        "    metrics = compute_metrics(\n",
        "        y_true=y_true_raw[model.target_names].values,\n",
        "        y_pred=pred_raw_df[model.target_names].values,\n",
        "        target_names=model.target_names,\n",
        "    )\n",
        "\n",
        "    indep_df = pd.DataFrame(metrics).T.reset_index().rename(columns={\"index\": \"target\"}) if metrics else pd.DataFrame()\n",
        "    display(indep_df)\n",
        "\n",
        "    indep_out = Path(\"results/metrics/colab_independent_validation_alpha_stable.csv\")\n",
        "    indep_df.to_csv(indep_out, index=False)\n",
        "    print(\"Saved\", indep_out)\n",
        "else:\n",
        "    print(\"Skipped independent validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Save outputs to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "drive_out = Path(f\"/content/drive/MyDrive/soil-resnet-outputs/full_pipeline_stable_{stamp}\")\n",
        "drive_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "copy_targets = [\n",
        "    paths[\"normalized_points\"],\n",
        "    paths[\"be_sgpt\"],\n",
        "    paths[\"sgpt_only\"],\n",
        "    paths[\"sgpt_raw\"],\n",
        "    paths[\"fused_feat\"],\n",
        "    paths[\"fused_meta\"],\n",
        "    paths[\"cfg_alpha\"],\n",
        "    paths[\"cfg_fused\"],\n",
        "    Path(\"results/metrics/colab_cv_summary_stable.csv\"),\n",
        "    Path(\"results/metrics/colab_independent_validation_alpha_stable.csv\"),\n",
        "]\n",
        "\n",
        "for p in copy_targets:\n",
        "    if p.exists():\n",
        "        shutil.copy2(p, drive_out / p.name)\n",
        "\n",
        "if paths[\"model_alpha\"].exists():\n",
        "    shutil.copytree(paths[\"model_alpha\"], drive_out / paths[\"model_alpha\"].name)\n",
        "if paths[\"model_fused\"].exists():\n",
        "    shutil.copytree(paths[\"model_fused\"], drive_out / paths[\"model_fused\"].name)\n",
        "\n",
        "print(\"Saved outputs to:\", drive_out)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "full_soil_pipeline_colab_stable.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}