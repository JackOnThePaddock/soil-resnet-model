{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Train ResNet From Precomputed Fused Data (Colab GPU)\n",
    "\n",
    "Run this notebook after Step 1.\n",
    "It trains/evaluates the stable normalized ResNet pipeline using precomputed Bare Earth + SpectralGPT fused features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime\n",
    "\n",
    "Set `Runtime -> Change runtime type -> T4/A100 GPU`, then run all cells in order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"[1/11] Mount + clone repo\")\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "USE_GIT_CLONE = True\n",
    "REPO_GIT_URL = \"https://github.com/JackOnThePaddock/soil-resnet-model.git\"\n",
    "DRIVE_REPO_DIR = \"/content/drive/MyDrive/soil-resnet-model\"\n",
    "PROJECT_DIR = \"/content/soil-resnet-model\"\n",
    "\n",
    "if os.path.exists(PROJECT_DIR):\n",
    "    shutil.rmtree(PROJECT_DIR)\n",
    "\n",
    "if USE_GIT_CLONE:\n",
    "    subprocess.run([\"git\", \"clone\", REPO_GIT_URL, PROJECT_DIR], check=True)\n",
    "else:\n",
    "    if not os.path.exists(DRIVE_REPO_DIR):\n",
    "        raise FileNotFoundError(f\"Repo not found at {DRIVE_REPO_DIR}\")\n",
    "    shutil.copytree(DRIVE_REPO_DIR, PROJECT_DIR)\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(\"Project dir:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[2/11] Install dependencies\")\n",
    "!python -V\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[3/11] Check GPU\")\n",
    "import torch\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[4/11] Configure run\")\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "RUN_ALPHA = True\n",
    "RUN_FUSED = True\n",
    "RUN_INDEPENDENT_VALIDATION = True\n",
    "FAST_MODE = False\n",
    "\n",
    "# If True, copy precomputed Step 1 files from Drive into this repo clone.\n",
    "COPY_PRECOMPUTED_FROM_DRIVE = True\n",
    "DRIVE_PRECOMPUTED_DIR = \"/content/drive/MyDrive/soil-resnet-outputs/step1_bareearth_sgpt_YYYYMMDD_HHMMSS\"\n",
    "\n",
    "TRAINING = {\n",
    "    \"ensemble_size\": 3 if FAST_MODE else 5,\n",
    "    \"n_splits\": 3 if FAST_MODE else 5,\n",
    "    \"epochs_alpha\": 60 if FAST_MODE else 300,\n",
    "    \"final_epochs_alpha\": 40 if FAST_MODE else 200,\n",
    "    \"epochs_fused\": 80 if FAST_MODE else 350,\n",
    "    \"final_epochs_fused\": 60 if FAST_MODE else 220,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 5e-5 if FAST_MODE else 8e-5,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"patience\": 20 if FAST_MODE else 35,\n",
    "    \"lr_patience\": 10 if FAST_MODE else 20,\n",
    "    \"lr_factor\": 0.5,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"random_seed\": 42,\n",
    "}\n",
    "\n",
    "paths = {\n",
    "    \"raw\": Path(\"data/processed/features.csv\"),\n",
    "    \"normalized_points\": Path(\"data/processed/features_normalized_points.csv\"),\n",
    "    \"be_sgpt\": Path(\"data/processed/features_normalized_points_be_sgpt.csv\"),\n",
    "    \"sgpt_only\": Path(\"data/processed/features_normalized_points_sgpt_embeddings.csv\"),\n",
    "    \"sgpt_raw\": Path(\"data/processed/features_normalized_points_sgpt_official_raw.csv\"),\n",
    "    \"fused_feat\": Path(\"data/processed/features_normalized_points_fused_feat.csv\"),\n",
    "    \"fused_meta\": Path(\"data/processed/features_normalized_points_fused_meta.json\"),\n",
    "    \"cfg_alpha\": Path(\"configs/colab_resnet_alpha_normalized_stable.yaml\"),\n",
    "    \"cfg_fused\": Path(\"configs/colab_resnet_fused_normalized_stable.yaml\"),\n",
    "    \"model_alpha\": Path(\"models/colab_resnet_alpha_norm_stable\"),\n",
    "    \"model_fused\": Path(\"models/colab_resnet_fused_norm_stable\"),\n",
    "    \"validation\": Path(\"data/validation/national_independent_1368.csv\"),\n",
    "}\n",
    "\n",
    "for req in [paths[\"raw\"]]:\n",
    "    if not req.exists():\n",
    "        raise FileNotFoundError(f\"Missing required input: {req}\")\n",
    "\n",
    "print(\"Config ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[5/11] Import Step 1 precomputed files (optional)\")\n",
    "if COPY_PRECOMPUTED_FROM_DRIVE:\n",
    "    drive_dir = Path(DRIVE_PRECOMPUTED_DIR)\n",
    "    if not drive_dir.exists():\n",
    "        raise FileNotFoundError(f\"DRIVE_PRECOMPUTED_DIR not found: {drive_dir}\")\n",
    "\n",
    "    required = [\n",
    "        paths[\"normalized_points\"].name,\n",
    "        paths[\"be_sgpt\"].name,\n",
    "        paths[\"sgpt_only\"].name,\n",
    "        paths[\"sgpt_raw\"].name,\n",
    "        paths[\"fused_feat\"].name,\n",
    "        paths[\"fused_meta\"].name,\n",
    "    ]\n",
    "    for name in required:\n",
    "        src = drive_dir / name\n",
    "        if not src.exists():\n",
    "            raise FileNotFoundError(f\"Missing precomputed file: {src}\")\n",
    "        dst = Path(\"data/processed\") / name\n",
    "        shutil.copy2(src, dst)\n",
    "        print(\"Copied\", src, \"->\", dst)\n",
    "else:\n",
    "    print(\"Using precomputed files already present in repo clone\")\n",
    "\n",
    "for req in [paths[\"normalized_points\"], paths[\"fused_feat\"], paths[\"fused_meta\"]]:\n",
    "    if not req.exists():\n",
    "        raise FileNotFoundError(f\"Missing required precomputed file: {req}\")\n",
    "\n",
    "print(\"Precomputed inputs ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[6/11] Write stable training configs\")\n",
    "# Clean model dirs for fresh run\n",
    "for d in [paths[\"model_alpha\"], paths[\"model_fused\"]]:\n",
    "    if d.exists():\n",
    "        shutil.rmtree(d)\n",
    "\n",
    "TARGETS = [\"ph\", \"cec\", \"esp\", \"soc\", \"ca\", \"mg\", \"na\"]\n",
    "\n",
    "fused_cols_df = pd.read_csv(paths[\"fused_feat\"], nrows=1)\n",
    "n_fused_features = len([c for c in fused_cols_df.columns if c.startswith(\"feat_\")])\n",
    "\n",
    "alpha_cfg = {\n",
    "    \"model\": {\"input_dim\": 64, \"hidden_dim\": 128, \"num_blocks\": 2, \"dropout\": 0.2, \"activation\": \"silu\"},\n",
    "    \"targets\": [\"pH\", \"CEC\", \"ESP\", \"SOC\", \"Ca\", \"Mg\", \"Na\"],\n",
    "    \"training\": {\n",
    "        \"ensemble_size\": TRAINING[\"ensemble_size\"],\n",
    "        \"n_splits\": TRAINING[\"n_splits\"],\n",
    "        \"epochs\": TRAINING[\"epochs_alpha\"],\n",
    "        \"final_epochs\": TRAINING[\"final_epochs_alpha\"],\n",
    "        \"batch_size\": TRAINING[\"batch_size\"],\n",
    "        \"learning_rate\": TRAINING[\"learning_rate\"],\n",
    "        \"weight_decay\": TRAINING[\"weight_decay\"],\n",
    "        \"patience\": TRAINING[\"patience\"],\n",
    "        \"lr_patience\": TRAINING[\"lr_patience\"],\n",
    "        \"lr_factor\": TRAINING[\"lr_factor\"],\n",
    "        \"grad_clip\": TRAINING[\"grad_clip\"],\n",
    "        \"cv_strategy\": \"group_kfold\",\n",
    "        \"loss_name\": \"weighted_huber\",\n",
    "        \"huber_delta\": 1.0,\n",
    "        \"esp_consistency_weight\": 0.0,\n",
    "        \"target_weight_mode\": \"inverse_frequency\",\n",
    "        \"sample_weight_mode\": \"rare_target_average\",\n",
    "        \"auto_target_transforms\": False,\n",
    "        \"target_transforms\": {t: \"identity\" for t in TARGETS},\n",
    "        \"specialist_targets\": [\"cec\", \"esp\", \"soc\"],\n",
    "        \"specialist_epochs\": 60 if FAST_MODE else 120,\n",
    "        \"specialist_patience\": 15 if FAST_MODE else 20,\n",
    "        \"specialist_val_fraction\": 0.2,\n",
    "        \"specialist_blend_weight\": 0.4,\n",
    "        \"random_seed\": TRAINING[\"random_seed\"],\n",
    "    },\n",
    "    \"data\": {\"feature_prefix\": \"band_\", \"n_features\": 64, \"group_by\": \"latlon\", \"group_round\": 4, \"reference_data\": None},\n",
    "    \"output\": {\"model_dir\": str(paths[\"model_alpha\"]), \"metrics_dir\": \"results/metrics\", \"scaler_file\": \"scaler.pkl\"},\n",
    "}\n",
    "\n",
    "fused_cfg = {\n",
    "    \"model\": {\"input_dim\": n_fused_features, \"hidden_dim\": 192, \"num_blocks\": 3, \"dropout\": 0.2, \"activation\": \"silu\"},\n",
    "    \"targets\": [\"pH\", \"CEC\", \"ESP\", \"SOC\", \"Ca\", \"Mg\", \"Na\"],\n",
    "    \"training\": {\n",
    "        \"ensemble_size\": TRAINING[\"ensemble_size\"],\n",
    "        \"n_splits\": TRAINING[\"n_splits\"],\n",
    "        \"epochs\": TRAINING[\"epochs_fused\"],\n",
    "        \"final_epochs\": TRAINING[\"final_epochs_fused\"],\n",
    "        \"batch_size\": TRAINING[\"batch_size\"],\n",
    "        \"learning_rate\": TRAINING[\"learning_rate\"],\n",
    "        \"weight_decay\": TRAINING[\"weight_decay\"],\n",
    "        \"patience\": TRAINING[\"patience\"],\n",
    "        \"lr_patience\": TRAINING[\"lr_patience\"],\n",
    "        \"lr_factor\": TRAINING[\"lr_factor\"],\n",
    "        \"grad_clip\": TRAINING[\"grad_clip\"],\n",
    "        \"cv_strategy\": \"group_kfold\",\n",
    "        \"loss_name\": \"weighted_huber\",\n",
    "        \"huber_delta\": 1.0,\n",
    "        \"esp_consistency_weight\": 0.0,\n",
    "        \"target_weight_mode\": \"inverse_frequency\",\n",
    "        \"sample_weight_mode\": \"rare_target_average\",\n",
    "        \"auto_target_transforms\": False,\n",
    "        \"target_transforms\": {t: \"identity\" for t in TARGETS},\n",
    "        \"specialist_targets\": [\"cec\", \"esp\", \"soc\"],\n",
    "        \"specialist_epochs\": 80 if FAST_MODE else 150,\n",
    "        \"specialist_patience\": 15 if FAST_MODE else 25,\n",
    "        \"specialist_val_fraction\": 0.2,\n",
    "        \"specialist_blend_weight\": 0.4,\n",
    "        \"random_seed\": TRAINING[\"random_seed\"],\n",
    "    },\n",
    "    \"data\": {\"feature_prefix\": \"feat_\", \"n_features\": None, \"group_by\": \"latlon\", \"group_round\": 4, \"reference_data\": None},\n",
    "    \"output\": {\"model_dir\": str(paths[\"model_fused\"]), \"metrics_dir\": \"results/metrics\", \"scaler_file\": \"scaler.pkl\"},\n",
    "}\n",
    "\n",
    "paths[\"cfg_alpha\"].write_text(yaml.safe_dump(alpha_cfg, sort_keys=False), encoding=\"utf-8\")\n",
    "paths[\"cfg_fused\"].write_text(yaml.safe_dump(fused_cfg, sort_keys=False), encoding=\"utf-8\")\n",
    "print(\"Wrote\", paths[\"cfg_alpha\"])\n",
    "print(\"Wrote\", paths[\"cfg_fused\"])\n",
    "print(\"Fused feature count:\", n_fused_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[7/11] Train alpha model\")\n",
    "if RUN_ALPHA:\n",
    "    cmd = [\n",
    "        \"python\", \"scripts/train_resnet_ensemble.py\",\n",
    "        \"--data\", str(paths[\"normalized_points\"]),\n",
    "        \"--config\", str(paths[\"cfg_alpha\"]),\n",
    "        \"--output\", str(paths[\"model_alpha\"]),\n",
    "        \"--cv-strategy\", \"group_kfold\",\n",
    "        \"--seed\", str(TRAINING[\"random_seed\"]),\n",
    "    ]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "else:\n",
    "    print(\"Skipped alpha training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[8/11] Train fused model\")\n",
    "if RUN_FUSED:\n",
    "    cmd = [\n",
    "        \"python\", \"scripts/train_resnet_ensemble.py\",\n",
    "        \"--data\", str(paths[\"fused_feat\"]),\n",
    "        \"--config\", str(paths[\"cfg_fused\"]),\n",
    "        \"--output\", str(paths[\"model_fused\"]),\n",
    "        \"--cv-strategy\", \"group_kfold\",\n",
    "        \"--seed\", str(TRAINING[\"random_seed\"]),\n",
    "    ]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "else:\n",
    "    print(\"Skipped fused training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[9/11] Validate checkpoints finite\")\n",
    "import torch\n",
    "\n",
    "\n",
    "def check_model_dir(model_dir: Path):\n",
    "    model_files = sorted(model_dir.glob(\"model_*.pth\"))\n",
    "    if not model_files:\n",
    "        raise FileNotFoundError(f\"No model_*.pth found in {model_dir}\")\n",
    "\n",
    "    print(\"Checking\", model_dir)\n",
    "    for mf in model_files:\n",
    "        ckpt = torch.load(mf, map_location=\"cpu\", weights_only=False)\n",
    "        state = ckpt[\"model_state_dict\"]\n",
    "        bad = []\n",
    "        for k, v in state.items():\n",
    "            if torch.is_tensor(v) and (torch.isnan(v).any() or torch.isinf(v).any()):\n",
    "                bad.append(k)\n",
    "        if bad:\n",
    "            raise RuntimeError(f\"Invalid weights in {mf.name}: {bad[:5]}\")\n",
    "        print(\"  OK\", mf.name)\n",
    "\n",
    "if RUN_ALPHA:\n",
    "    check_model_dir(paths[\"model_alpha\"])\n",
    "if RUN_FUSED:\n",
    "    check_model_dir(paths[\"model_fused\"])\n",
    "print(\"All checkpoints finite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[10/11] CV summary + independent validation\")\n",
    "import re\n",
    "\n",
    "\n",
    "def summarize_ensemble_metrics(csv_path: Path, model_label: str) -> pd.DataFrame:\n",
    "    if not csv_path.exists():\n",
    "        return pd.DataFrame([{\"model\": model_label, \"target\": \"(missing)\", \"metric\": \"\", \"mean\": np.nan, \"std\": np.nan}])\n",
    "    df = pd.read_csv(csv_path)\n",
    "    rows = []\n",
    "    for col in df.columns:\n",
    "        m = re.match(r\"^(.*)_(r2|rmse|mae)$\", col)\n",
    "        if m:\n",
    "            rows.append({\n",
    "                \"model\": model_label,\n",
    "                \"target\": m.group(1).lower(),\n",
    "                \"metric\": m.group(2),\n",
    "                \"mean\": float(df[col].mean()),\n",
    "                \"std\": float(df[col].std(ddof=0)),\n",
    "            })\n",
    "    if not rows:\n",
    "        rows.append({\"model\": model_label, \"target\": \"(no per-target metrics)\", \"metric\": \"\", \"mean\": np.nan, \"std\": np.nan})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "alpha_cv = summarize_ensemble_metrics(paths[\"model_alpha\"] / \"ensemble_metrics.csv\", \"alpha_norm_stable\")\n",
    "fused_cv = summarize_ensemble_metrics(paths[\"model_fused\"] / \"ensemble_metrics.csv\", \"fused_norm_stable\")\n",
    "cv_summary = pd.concat([alpha_cv, fused_cv], ignore_index=True)\n",
    "display(cv_summary)\n",
    "\n",
    "cv_out = Path(\"results/metrics/colab_cv_summary_train_only.csv\")\n",
    "cv_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "cv_summary.to_csv(cv_out, index=False)\n",
    "print(\"Saved\", cv_out)\n",
    "\n",
    "if RUN_INDEPENDENT_VALIDATION:\n",
    "    from src.models.ensemble import SoilEnsemble\n",
    "    from src.evaluation.metrics import compute_metrics\n",
    "\n",
    "    val_path = paths[\"validation\"]\n",
    "    if not val_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing validation file: {val_path}\")\n",
    "\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    for i in range(64):\n",
    "        src = f\"A{i:02d}\"\n",
    "        if src in val_df.columns:\n",
    "            val_df[f\"band_{i}\"] = val_df[src]\n",
    "\n",
    "    feature_cols = [f\"band_{i}\" for i in range(64)]\n",
    "    missing = [c for c in feature_cols if c not in val_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Validation missing feature columns: {missing[:5]}\")\n",
    "\n",
    "    model = SoilEnsemble(paths[\"model_alpha\"])\n",
    "    pred_norm, _ = model.predict_batch(val_df[feature_cols].values.astype(np.float32))\n",
    "\n",
    "    targets = [\"ph\", \"cec\", \"esp\", \"soc\", \"ca\", \"mg\", \"na\"]\n",
    "    train_raw = pd.read_csv(paths[\"raw\"], usecols=targets)\n",
    "    mu = train_raw.mean(axis=0)\n",
    "    sigma = train_raw.std(axis=0, ddof=0).replace(0.0, 1.0)\n",
    "\n",
    "    pred_norm_df = pd.DataFrame(pred_norm, columns=model.target_names)\n",
    "    pred_raw_df = pred_norm_df.copy()\n",
    "    for t in model.target_names:\n",
    "        pred_raw_df[t] = pred_norm_df[t] * float(sigma[t]) + float(mu[t])\n",
    "\n",
    "    y_true_raw = pd.DataFrame(np.nan, index=val_df.index, columns=model.target_names)\n",
    "    truth_map = {\"ph\": \"ph\", \"cec\": \"cec_cmolkg\", \"esp\": \"esp_pct\", \"na\": \"na_cmolkg\"}\n",
    "    for t, src_col in truth_map.items():\n",
    "        if src_col in val_df.columns:\n",
    "            y_true_raw[t] = pd.to_numeric(val_df[src_col], errors=\"coerce\")\n",
    "\n",
    "    metrics = compute_metrics(\n",
    "        y_true=y_true_raw[model.target_names].values,\n",
    "        y_pred=pred_raw_df[model.target_names].values,\n",
    "        target_names=model.target_names,\n",
    "    )\n",
    "\n",
    "    indep_df = pd.DataFrame(metrics).T.reset_index().rename(columns={\"index\": \"target\"}) if metrics else pd.DataFrame()\n",
    "    display(indep_df)\n",
    "\n",
    "    indep_out = Path(\"results/metrics/colab_independent_validation_alpha_train_only.csv\")\n",
    "    indep_df.to_csv(indep_out, index=False)\n",
    "    print(\"Saved\", indep_out)\n",
    "else:\n",
    "    print(\"Skipped independent validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[11/11] Save outputs to Drive\")\n",
    "from datetime import datetime\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "drive_out = Path(f\"/content/drive/MyDrive/soil-resnet-outputs/step2_resnet_train_{stamp}\")\n",
    "drive_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "copy_targets = [\n",
    "    paths[\"cfg_alpha\"],\n",
    "    paths[\"cfg_fused\"],\n",
    "    Path(\"results/metrics/colab_cv_summary_train_only.csv\"),\n",
    "    Path(\"results/metrics/colab_independent_validation_alpha_train_only.csv\"),\n",
    "]\n",
    "\n",
    "for p in copy_targets:\n",
    "    if p.exists():\n",
    "        shutil.copy2(p, drive_out / p.name)\n",
    "\n",
    "if paths[\"model_alpha\"].exists():\n",
    "    shutil.copytree(paths[\"model_alpha\"], drive_out / paths[\"model_alpha\"].name)\n",
    "if paths[\"model_fused\"].exists():\n",
    "    shutil.copytree(paths[\"model_fused\"], drive_out / paths[\"model_fused\"].name)\n",
    "\n",
    "print(\"Saved outputs to:\", drive_out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
